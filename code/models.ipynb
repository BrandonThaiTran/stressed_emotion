{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opposite-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sampler\n",
    "import datasets\n",
    "from earlystopping import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from torch.autograd  import  Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-desperate",
   "metadata": {},
   "source": [
    "# Acoustic Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-america",
   "metadata": {},
   "source": [
    "Inputs for acoustic branch will be N x 40 where N [1,33]  \n",
    "Time step: (2, 10) (seconds?)  \n",
    "N: relative duration after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beneficial-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticNet(nn.Module):\n",
    "    def __init__(self, num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2):\n",
    "        super(AcousticNet, self).__init__()\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.convs = [self.conv1, self.conv2, self.conv3, self.conv4]\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=conv_width,hidden_size=32,num_layers=num_gru_layers) # 19 is hardcoded\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "#         print(x.shape)\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.relu(self.max_pool(self.convs[i](x)))\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = F.adaptive_avg_pool1d(x,1)[:, :, -1]\n",
    "#         x = self.mean_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mediterranean-internet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = AcousticNet(num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "test_vec = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "output = net(test_vec)\n",
    "print(f'Shape of output: {output.shape}')\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-allen",
   "metadata": {},
   "source": [
    "# Lexical Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sitting-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement GRU (or transformer)\n",
    "class LexicalNet(nn.Module):\n",
    "    def __init__(self, num_gru_layers = 2):\n",
    "        super(LexicalNet, self).__init__()\n",
    "        # implement GRU (or transformer)\n",
    "        self.gru = nn.GRU(input_size=768,hidden_size=32,num_layers=num_gru_layers)\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2) \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "#         x = self.mean_pool(x)\n",
    "        x = self.flatten(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crazy-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dummy input\n",
    "net = LexicalNet(num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "test_vec = torch.randn(batch_size, 1, 768)\n",
    "output = net(test_vec)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-optimum",
   "metadata": {},
   "source": [
    "# Master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "velvet-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRL(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self,x):\n",
    "#         return x\n",
    "#     @staticmethod\n",
    "#     def backward(self,grad_output):\n",
    "#         grad_input = grad_output.neg()\n",
    "#         return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "humanitarian-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "    \n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "large-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterNet(nn.Module):\n",
    "    def __init__(self, acoustic_modality = True, lexical_modality = True, visual_modality = False,\n",
    "                 num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2,\n",
    "                 num_dense_layers = 1, dense_layer_width = 32, grl_lambda = .3):\n",
    "        super(MasterNet, self).__init__()\n",
    "        \n",
    "        self.acoustic_modality = acoustic_modality\n",
    "        self.lexical_modality = lexical_modality\n",
    "        self.visual_modality = visual_modality\n",
    "        \n",
    "        self.acoustic_model = AcousticNet(num_conv_layers = num_conv_layers, kernel_size = kernel_size, \n",
    "                                     conv_width = conv_width, num_gru_layers = num_gru_layers)\n",
    "        self.lexical_model = LexicalNet(num_gru_layers = 2)\n",
    "        \n",
    "        # emotion classifier\n",
    "#         self.dense1_emo = nn.Linear()\n",
    "#         self.dense2_emo = nn.Linear()\n",
    "        \n",
    "        width = 0 # width of the FC layers\n",
    "        if self.acoustic_modality:\n",
    "            width += 32\n",
    "        if self.visual_modality:\n",
    "            width += 0 # to implement\n",
    "        if self.lexical_modality:\n",
    "            width += 32\n",
    "            \n",
    "        self.fc_1 = nn.Linear(width, dense_layer_width)\n",
    "        self.fc_2 = nn.Linear(dense_layer_width, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "#         # To implement   \n",
    "#         if num_dense_layers == 2:\n",
    "#             self.fc = nn.Sequential()\n",
    "#             self.linear_1 = nn.Linear(width, dense_layer_width)\n",
    "#         else:\n",
    "#             self.fc = \n",
    "        \n",
    "        # confound classifier -- to implement\n",
    "        self.grl = GradientReversal(lambda_ = -1)\n",
    "        self.dense_con = nn.Linear(width, 2)\n",
    "#         self.dense2_con = None\n",
    "        \n",
    "        \n",
    "    def forward_a(self, x_a):\n",
    "        x = x_a\n",
    "        x = self.acoustic_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_l(self, x_l):\n",
    "        x = torch.unsqueeze(x_l, dim = 1)\n",
    "        x = self.lexical_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_v(self, x_v):\n",
    "        x = x_v\n",
    "        return x\n",
    "    \n",
    "    def encoder(self, x_v, x_a, x_l):\n",
    "        if self.visual_modality:\n",
    "            x_v = self.forward_v(x_v)\n",
    "        if self.acoustic_modality:\n",
    "            x_a = self.forward_a(x_a)\n",
    "        if self.lexical_modality:\n",
    "            x_l = self.forward_l(x_l)\n",
    "        \n",
    "        if self.visual_modality:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = torch.cat((x_v, x_a), 1)\n",
    "            else:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_l), 1)\n",
    "                else:\n",
    "                    x = x_v\n",
    "        else:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = x_a\n",
    "            else:\n",
    "                x = x_l\n",
    "        return x\n",
    "\n",
    "    def confound_model(self, x):\n",
    "#         x = self.grl.apply(x)\n",
    "        x = self.grl(x)\n",
    "        x = self.dense_con(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def recognizer(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_v, x_a, x_l):\n",
    "        x = self.encoder(x_v, x_a, x_l)\n",
    "        emotion_output = self.recognizer(x)\n",
    "        confound_output = self.confound_model(x)\n",
    "        \n",
    "        return emotion_output, confound_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handmade-starter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of emotion output: torch.Size([8, 3])\n",
      "Shape of stress output: torch.Size([8, 2])\n",
      "tensor([[0.3481, 0.2975, 0.3545],\n",
      "        [0.3536, 0.2962, 0.3503],\n",
      "        [0.3468, 0.2990, 0.3541],\n",
      "        [0.3346, 0.3055, 0.3599],\n",
      "        [0.3427, 0.2904, 0.3668],\n",
      "        [0.3580, 0.2844, 0.3576],\n",
      "        [0.3666, 0.2851, 0.3484],\n",
      "        [0.3648, 0.2780, 0.3573]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4836, 0.5164],\n",
      "        [0.5079, 0.4921],\n",
      "        [0.4559, 0.5441],\n",
      "        [0.4443, 0.5557],\n",
      "        [0.4038, 0.5962],\n",
      "        [0.4272, 0.5728],\n",
      "        [0.4376, 0.5624],\n",
      "        [0.4299, 0.5701]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = MasterNet()\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "acoustic_features = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "# lexical_features = torch.randn(batch_size, 1, 300)\n",
    "lexical_features = torch.randn(batch_size, 768)\n",
    "visual_features = None\n",
    "emotion_output, stress_output = net(visual_features, acoustic_features, lexical_features)\n",
    "print(f'Shape of emotion output: {emotion_output.shape}')\n",
    "print(f'Shape of stress output: {stress_output.shape}')\n",
    "print(emotion_output)\n",
    "print(stress_output)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "generous-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use specific GPU\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():  \n",
    "        dev = \"cuda:0\" \n",
    "    else:  \n",
    "        dev = \"cpu\"  \n",
    "    return torch.device(dev)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "typical-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_folder(model, folder = 0, epochs = 1, verbose = False, learning_rate = 1e-4, patience = 5):\n",
    "    # Use specific GPU\n",
    "    device = get_device()\n",
    "\n",
    "    # Dataloaders    \n",
    "    train_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/train.csv'\n",
    "    train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "    test_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/test.csv'\n",
    "    test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "    # Model, optimizer and loss function\n",
    "    emotion_recognizer = model\n",
    "    init_weights(emotion_recognizer)\n",
    "    for param in emotion_recognizer.parameters():\n",
    "        param.requires_grad = True\n",
    "    emotion_recognizer.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "    lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.\n",
    "    best_uar = 0.\n",
    "    es = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Train and validate\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print('epoch: {}/{}'.format(epoch + 1, epochs))\n",
    "\n",
    "        train_loss, train_acc = train(train_loader, emotion_recognizer,\n",
    "                                        optimizer, criterion, device)\n",
    "        test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer,\n",
    "                                                criterion, device)\n",
    "\n",
    "        if verbose:\n",
    "            print('train_loss: {0:.5f}'.format(train_loss['loss']),\n",
    "                  'train_acc: {0:.3f}'.format(train_acc['avg_acc']),\n",
    "                  'test_loss: {0:.5f}'.format(test_loss['loss']),\n",
    "                  'test_acc: {0:.3f}'.format(test_acc['avg_acc']),\n",
    "                  'test_uar: {0:.3f}'.format(test_uar['avg_uar']))\n",
    "\n",
    "        lr_schedule.step(test_loss['loss'])\n",
    "\n",
    "#         os.makedirs(os.path.join(opt.logger_path, opt.source_domain), exist_ok=True)\n",
    "\n",
    "#         model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'checkpoint.pth.tar')\n",
    "#         state = {'epoch': epoch+1, 'emotion_recognizer': emotion_recognizer.state_dict(), 'opt': opt}\n",
    "#         torch.save(state, model_file_name)\n",
    "\n",
    "        if test_acc['avg_acc'] > best_acc:\n",
    "#             model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'model.pth.tar')\n",
    "#             torch.save(state, model_file_name)\n",
    "\n",
    "            best_acc = test_acc['avg_acc']\n",
    "\n",
    "        if test_uar['avg_uar'] > best_uar:\n",
    "            best_uar = test_uar['avg_uar']\n",
    "\n",
    "        if es.step(test_loss['loss']):\n",
    "            break\n",
    "\n",
    "    return best_acc, best_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hungarian-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = 'train_loss: {0:.5f}'.format(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vocational-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 123.00000\n",
      "train_loss: 0.00000 train_acc: 1.000 test_loss: 2.00000 test_acc: 3.000 test_uar: 4.000\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0 \n",
    "train_acc = 1\n",
    "test_loss = 2\n",
    "test_acc = 3\n",
    "test_uar = 4\n",
    "hi = 'train_loss: {0:.5f}'.format(123)\n",
    "print(hi)\n",
    "print('train_loss: {0:.5f}'.format(train_loss),\n",
    "                    'train_acc: {0:.3f}'.format(train_acc),\n",
    "                    'test_loss: {val:.5f}'.format(val=test_loss),\n",
    "                    'test_acc: {val:.3f}'.format(val=test_acc),\n",
    "                    'test_uar: {0:.3f}'.format(test_uar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "second-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just emotion task\n",
    "\n",
    "# def train(train_loader, model, optimizer, criterion, device, verbose = False):\n",
    "#     model.train()\n",
    "\n",
    "#     running_loss = 0.\n",
    "#     running_acc = 0.\n",
    "\n",
    "#     groundtruth = []\n",
    "#     prediction = []\n",
    "\n",
    "#     for i, train_data in enumerate(train_loader):\n",
    "#         visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = train_data # UPDATE\n",
    "\n",
    "#         visual_features = visual_features.to(device)\n",
    "#         acoustic_features = acoustic_features.to(device)\n",
    "#         lexical_features = lexical_features.to(device)\n",
    "\n",
    "#         labels = a_labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         emotion_output, stress_output = model(visual_features, acoustic_features, lexical_features)\n",
    "\n",
    "#         emotion_loss = criterion(emotion_output, labels)\n",
    "# #         stress_loss = criterion(stress_output, stress_labels)\n",
    "\n",
    "#         emotion_loss.backward()\n",
    "# #         stress_loss.backward()\n",
    "        \n",
    "#         optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "#         running_loss += emotion_loss.item()\n",
    "\n",
    "#         groundtruth.append(labels.tolist())\n",
    "#         predictions = emotion_output.argmax(dim=1, keepdim=True)\n",
    "#         prediction.append(predictions.view_as(labels).tolist())\n",
    "\n",
    "#         if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "#             print('.', flush=True, end='')\n",
    "            \n",
    "#     train_loss = running_loss / len(train_loader)\n",
    "\n",
    "#     groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "#     prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "#     train_acc = accuracy_score(prediction, groundtruth)\n",
    "\n",
    "#     return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "heard-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, criterion, device, verbose = False):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    emotion_running_loss = 0.\n",
    "    confound_running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    emotion_groundtruth = []\n",
    "    emotion_prediction = []\n",
    "    \n",
    "    confound_groundtruth = []\n",
    "    confound_prediction = []\n",
    "\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, d_labels, s_labels, _ = train_data # UPDATE\n",
    "\n",
    "        visual_features = visual_features.to(device)\n",
    "        acoustic_features = acoustic_features.to(device)\n",
    "        lexical_features = lexical_features.to(device)\n",
    "\n",
    "        emotion_labels = a_labels.to(device)\n",
    "        confound_labels = s_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "\n",
    "        emotion_loss = criterion(emotion_predictions, emotion_labels)\n",
    "        confound_loss = criterion(confound_predictions, confound_labels)\n",
    "        loss = emotion_loss + confound_loss\n",
    "        \n",
    "        loss.backward()\n",
    "#         emotion_loss.backward()\n",
    "#         confound_loss.backward()\n",
    "        \n",
    "        optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "        emotion_running_loss += emotion_loss.item()\n",
    "        confound_running_loss += confound_loss.item()\n",
    "        running_loss += emotion_running_loss + confound_running_loss\n",
    "\n",
    "        emotion_groundtruth.append(emotion_labels.tolist())\n",
    "        emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "        emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\n",
    "        \n",
    "        confound_groundtruth.append(confound_labels.tolist())\n",
    "        confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\n",
    "        confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\n",
    "\n",
    "        if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "            print('.', flush=True, end='')\n",
    "        \n",
    "    emotion_loss = emotion_running_loss / len(train_loader)\n",
    "    confound_loss = confound_running_loss / len(train_loader)\n",
    "    loss = running_loss / len(train_loader)\n",
    "    train_loss = {'emotion_loss': emotion_loss,\n",
    "                  'confound_loss': confound_loss,\n",
    "                  'loss': loss\n",
    "                 }\n",
    "\n",
    "    emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\n",
    "    emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\n",
    "    \n",
    "    confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\n",
    "    confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\n",
    "\n",
    "    emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\n",
    "    confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\n",
    "    avg_acc = (emotion_acc + confound_acc) / 2\n",
    "    \n",
    "    train_acc = {'emotion_acc': emotion_acc,\n",
    "                  'confound_acc': confound_acc,\n",
    "                  'avg_acc': avg_acc\n",
    "                 }\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sacred-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old test loss \n",
    "# def test(test_loader, model, criterion, device):\n",
    "#     model.eval()\n",
    "\n",
    "#     running_loss = 0.\n",
    "#     running_acc = 0.\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         groundtruth = []\n",
    "#         prediction = []\n",
    "\n",
    "#         for i, test_data in enumerate(test_loader):\n",
    "#             visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, _ = test_data # UPDATE\n",
    "\n",
    "#             visual_features = visual_features.to(device)\n",
    "#             acoustic_features = acoustic_features.to(device)\n",
    "#             lexical_features = lexical_features.to(device)\n",
    "\n",
    "#             labels = a_labels.to(device)\n",
    "\n",
    "#             emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "#             loss = criterion(emotion_predictions, labels)\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             groundtruth.append(labels.tolist())\n",
    "#             emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "#             prediction.append(emotion_predictions.view_as(labels).tolist())\n",
    "\n",
    "#         test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#         groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "#         prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "#         test_acc = accuracy_score(prediction, groundtruth)\n",
    "#         test_uar = recall_score(prediction, groundtruth, average='macro')\n",
    "\n",
    "#         return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "gorgeous-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.\n",
    "    emotion_running_loss = 0.\n",
    "    confound_running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotion_groundtruth = []\n",
    "        emotion_prediction = []\n",
    "\n",
    "        confound_groundtruth = []\n",
    "        confound_prediction = []\n",
    "\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, s_labels, _ = test_data # UPDATE\n",
    "\n",
    "            visual_features = visual_features.to(device)\n",
    "            acoustic_features = acoustic_features.to(device)\n",
    "            lexical_features = lexical_features.to(device)\n",
    "\n",
    "            emotion_labels = a_labels.to(device)\n",
    "            confound_labels = s_labels.to(device)\n",
    "\n",
    "            emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "            \n",
    "            emotion_loss = criterion(emotion_predictions, emotion_labels)\n",
    "            confound_loss = criterion(confound_predictions, confound_labels)\n",
    "            loss = emotion_loss + confound_loss\n",
    "\n",
    "            emotion_running_loss += emotion_loss.item()\n",
    "            confound_running_loss += confound_loss.item()\n",
    "            running_loss += emotion_running_loss + confound_running_loss\n",
    "\n",
    "            emotion_groundtruth.append(emotion_labels.tolist())\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "            emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\n",
    "\n",
    "            confound_groundtruth.append(confound_labels.tolist())\n",
    "            confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\n",
    "            confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\n",
    "\n",
    "        emotion_loss = emotion_running_loss / len(train_loader)\n",
    "        confound_loss = confound_running_loss / len(train_loader)\n",
    "        loss = running_loss / len(train_loader)\n",
    "        test_loss = {'emotion_loss': emotion_loss,\n",
    "                     'confound_loss': confound_loss,\n",
    "                     'loss': loss\n",
    "                    }\n",
    "\n",
    "        emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\n",
    "        emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\n",
    "\n",
    "        confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\n",
    "        confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\n",
    "\n",
    "        emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\n",
    "        confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\n",
    "        avg_acc = (emotion_acc + confound_acc) / 2\n",
    "        test_acc = {'emotion_acc': emotion_acc,\n",
    "                    'confound_acc': confound_acc,\n",
    "                    'avg_acc': avg_acc\n",
    "                   }\n",
    "        \n",
    "        emotion_uar = recall_score(emotion_prediction, emotion_groundtruth, average='macro')\n",
    "        confound_uar = recall_score(confound_prediction, confound_groundtruth, average='macro')\n",
    "\n",
    "        avg_uar = (emotion_uar + confound_acc) / 2\n",
    "        test_uar = {'emotion_uar': emotion_uar,\n",
    "                    'confound_uar': confound_uar,\n",
    "                    'avg_uar': avg_uar\n",
    "                   }\n",
    "\n",
    "        return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "republican-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "secret-statistics",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# emotion_recognizer = net.Model(opt)\n",
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(train_acc)\n",
    "\n",
    "plt.plot(loss, label = 'train loss')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'emotion loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'confound loss')\n",
    "plt.legend()\n",
    "\n",
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "backed-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "uar = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "naked-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 914.49928 train_acc: 0.495 test_loss: 42.63773 test_acc: 0.636 test_uar: 0.593\n",
      "epoch: 2/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 865.53035 train_acc: 0.588 test_loss: 40.41301 test_acc: 0.648 test_uar: 0.634\n",
      "epoch: 3/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 786.84790 train_acc: 0.672 test_loss: 37.77749 test_acc: 0.715 test_uar: 0.656\n",
      "epoch: 4/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 743.74327 train_acc: 0.700 test_loss: 36.88711 test_acc: 0.720 test_uar: 0.691\n",
      "epoch: 5/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 726.67255 train_acc: 0.721 test_loss: 36.61339 test_acc: 0.715 test_uar: 0.690\n",
      "epoch: 6/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 710.09633 train_acc: 0.732 test_loss: 36.21433 test_acc: 0.719 test_uar: 0.693\n",
      "epoch: 7/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 707.11552 train_acc: 0.737 test_loss: 36.17058 test_acc: 0.716 test_uar: 0.702\n",
      "epoch: 8/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 702.35185 train_acc: 0.739 test_loss: 36.00136 test_acc: 0.717 test_uar: 0.701\n",
      "epoch: 9/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 692.36953 train_acc: 0.748 test_loss: 35.89804 test_acc: 0.719 test_uar: 0.694\n",
      "epoch: 10/10\n",
      "confound train\n",
      "[0 1]\n",
      "[0 1]\n",
      "confound test\n",
      "[0 1]\n",
      "[0 1]\n",
      "train_loss: 687.42408 train_acc: 0.755 test_loss: 36.21142 test_acc: 0.707 test_uar: 0.703\n"
     ]
    }
   ],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 0, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 1, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 2, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 3, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 4, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(5),acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(5),uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([acc,uar]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-capital",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
