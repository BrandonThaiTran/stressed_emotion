{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "early-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sampler\n",
    "import datasets\n",
    "from earlystopping import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from torch.autograd  import  Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-discrimination",
   "metadata": {},
   "source": [
    "# Acoustic Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-shepherd",
   "metadata": {},
   "source": [
    "Inputs for acoustic branch will be N x 40 where N [1,33]  \n",
    "Time step: (2, 10) (seconds?)  \n",
    "N: relative duration after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prostate-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticNet(nn.Module):\n",
    "    def __init__(self, num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2):\n",
    "        super(AcousticNet, self).__init__()\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.convs = [self.conv1, self.conv2, self.conv3, self.conv4]\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=conv_width,hidden_size=32,num_layers=num_gru_layers) # 19 is hardcoded\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "#         print(x.shape)\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.relu(self.max_pool(self.convs[i](x)))\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = F.adaptive_avg_pool1d(x,1)[:, :, -1]\n",
    "#         x = self.mean_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "small-palestinian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = AcousticNet(num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "test_vec = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "output = net(test_vec)\n",
    "print(f'Shape of output: {output.shape}')\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-julian",
   "metadata": {},
   "source": [
    "# Lexical Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "civic-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement GRU (or transformer)\n",
    "class LexicalNet(nn.Module):\n",
    "    def __init__(self, num_gru_layers = 2):\n",
    "        super(LexicalNet, self).__init__()\n",
    "        # implement GRU (or transformer)\n",
    "        self.gru = nn.GRU(input_size=768,hidden_size=32,num_layers=num_gru_layers)\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2) \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "#         x = self.mean_pool(x)\n",
    "        x = self.flatten(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "human-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dummy input\n",
    "net = LexicalNet(num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "test_vec = torch.randn(batch_size, 1, 768)\n",
    "output = net(test_vec)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-integral",
   "metadata": {},
   "source": [
    "# Master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "medium-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRL(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self,x):\n",
    "#         return x\n",
    "#     @staticmethod\n",
    "#     def backward(self,grad_output):\n",
    "#         grad_input = grad_output.neg()\n",
    "#         return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developing-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "    \n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "junior-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterNet(nn.Module):\n",
    "    def __init__(self, acoustic_modality = True, lexical_modality = True, visual_modality = False,\n",
    "                 num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2,\n",
    "                 num_dense_layers = 1, dense_layer_width = 32, grl_lambda = .3):\n",
    "        super(MasterNet, self).__init__()\n",
    "        \n",
    "        self.acoustic_modality = acoustic_modality\n",
    "        self.lexical_modality = lexical_modality\n",
    "        self.visual_modality = visual_modality\n",
    "        \n",
    "        self.acoustic_model = AcousticNet(num_conv_layers = num_conv_layers, kernel_size = kernel_size, \n",
    "                                     conv_width = conv_width, num_gru_layers = num_gru_layers)\n",
    "        self.lexical_model = LexicalNet(num_gru_layers = 2)\n",
    "        \n",
    "        # emotion classifier\n",
    "#         self.dense1_emo = nn.Linear()\n",
    "#         self.dense2_emo = nn.Linear()\n",
    "        \n",
    "        width = 0 # width of the FC layers\n",
    "        if self.acoustic_modality:\n",
    "            width += 32\n",
    "        if self.visual_modality:\n",
    "            width += 0 # to implement\n",
    "        if self.lexical_modality:\n",
    "            width += 32\n",
    "            \n",
    "        self.fc_1 = nn.Linear(width, dense_layer_width)\n",
    "        self.fc_2 = nn.Linear(dense_layer_width, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "#         # To implement   \n",
    "#         if num_dense_layers == 2:\n",
    "#             self.fc = nn.Sequential()\n",
    "#             self.linear_1 = nn.Linear(width, dense_layer_width)\n",
    "#         else:\n",
    "#             self.fc = \n",
    "        \n",
    "        # confound classifier -- to implement\n",
    "        self.grl = GradientReversal(lambda_ = grl_lambda)\n",
    "        self.dense_conv = nn.Linear(width, 2)\n",
    "#         self.dense2_con = None\n",
    "        \n",
    "        \n",
    "    def forward_a(self, x_a):\n",
    "        x = x_a\n",
    "        x = self.acoustic_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_l(self, x_l):\n",
    "        x = torch.unsqueeze(x_l, dim = 1)\n",
    "        x = self.lexical_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_v(self, x_v):\n",
    "        x = x_v\n",
    "        return x\n",
    "    \n",
    "    def encoder(self, x_v, x_a, x_l):\n",
    "        if self.visual_modality:\n",
    "            x_v = self.forward_v(x_v)\n",
    "        if self.acoustic_modality:\n",
    "            x_a = self.forward_a(x_a)\n",
    "        if self.lexical_modality:\n",
    "            x_l = self.forward_l(x_l)\n",
    "        \n",
    "        if self.visual_modality:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = torch.cat((x_v, x_a), 1)\n",
    "            else:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_l), 1)\n",
    "                else:\n",
    "                    x = x_v\n",
    "        else:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = x_a\n",
    "            else:\n",
    "                x = x_l\n",
    "        return x\n",
    "\n",
    "    def confound_model(self, x):\n",
    "#         x = self.grl.apply(x)\n",
    "        x = self.grl(x)\n",
    "        x = self.dense_conv(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    # For emotion\n",
    "    def recognizer(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_v, x_a, x_l):\n",
    "        x = self.encoder(x_v, x_a, x_l)\n",
    "        emotion_output = self.recognizer(x)\n",
    "        confound_output = self.confound_model(x)\n",
    "        \n",
    "        return emotion_output, confound_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sweet-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of emotion output: torch.Size([8, 3])\n",
      "Shape of stress output: torch.Size([8, 2])\n",
      "tensor([[0.3390, 0.3572, 0.3038],\n",
      "        [0.3378, 0.3475, 0.3147],\n",
      "        [0.3361, 0.3413, 0.3226],\n",
      "        [0.3340, 0.3444, 0.3216],\n",
      "        [0.3343, 0.3456, 0.3201],\n",
      "        [0.3319, 0.3386, 0.3295],\n",
      "        [0.3379, 0.3409, 0.3212],\n",
      "        [0.3427, 0.3499, 0.3074]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4837, 0.5163],\n",
      "        [0.4901, 0.5099],\n",
      "        [0.4634, 0.5366],\n",
      "        [0.4529, 0.5471],\n",
      "        [0.4420, 0.5580],\n",
      "        [0.4342, 0.5658],\n",
      "        [0.4193, 0.5807],\n",
      "        [0.4254, 0.5746]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = MasterNet()\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "acoustic_features = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "# lexical_features = torch.randn(batch_size, 1, 300)\n",
    "lexical_features = torch.randn(batch_size, 768)\n",
    "visual_features = None\n",
    "emotion_output, stress_output = net(visual_features, acoustic_features, lexical_features)\n",
    "print(f'Shape of emotion output: {emotion_output.shape}')\n",
    "print(f'Shape of stress output: {stress_output.shape}')\n",
    "print(emotion_output)\n",
    "print(stress_output)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "destroyed-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use specific GPU\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():  \n",
    "        dev = \"cuda:0\" \n",
    "    else:  \n",
    "        dev = \"cpu\"  \n",
    "    return torch.device(dev)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scheduled-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_folder(model, folder = 0, epochs = 1, verbose = False, learning_rate = 1e-4, patience = 5):\n",
    "    # Use specific GPU\n",
    "    device = get_device()\n",
    "\n",
    "    # Dataloaders    \n",
    "    train_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/train.csv'\n",
    "    train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "    test_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/test.csv'\n",
    "    test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "    # Model, optimizer and loss function\n",
    "    init_weights(model)\n",
    "    for param in emotion_recognizer.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.\n",
    "    best_uar = 0.\n",
    "    es = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Train and validate\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print('epoch: {}/{}'.format(epoch + 1, epochs))\n",
    "\n",
    "        train_loss, train_acc = train(train_loader, model,\n",
    "                                        optimizer, criterion, device)\n",
    "        test_loss, test_acc, test_uar = test(test_loader, model,\n",
    "                                                criterion, device)\n",
    "\n",
    "        if verbose:\n",
    "            print('train_emotion_loss: {0:.5f}'.format(train_loss['emotion_loss']),\n",
    "                  'train_emotion_acc: {0:.3f}'.format(train_acc['emotion_acc']),\n",
    "                  'train_confound_loss: {0:.5f}'.format(train_loss['confound_loss']),\n",
    "                  'train_confound_acc: {0:.3f}'.format(train_acc['confound_acc']),\n",
    "                  'test_emotion_loss: {0:.5f}'.format(test_loss['emotion_loss']),\n",
    "                  'test_emotion_acc: {0:.3f}'.format(test_acc['emotion_acc']),\n",
    "                  'test_confound_loss: {0:.5f}'.format(test_loss['confound_loss']),\n",
    "                  'test_confound_acc: {0:.3f}'.format(test_acc['confound_acc']),\n",
    "                  'test_emotion_uar: {0:.3f}'.format(test_uar['emotion_uar']),\n",
    "                  'test_confound_uar: {0:.3f}'.format(test_uar['confound_uar']))\n",
    "\n",
    "        lr_schedule.step(test_loss['loss'])\n",
    "\n",
    "#         os.makedirs(os.path.join(opt.logger_path, opt.source_domain), exist_ok=True)\n",
    "\n",
    "#         model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'checkpoint.pth.tar')\n",
    "#         state = {'epoch': epoch+1, 'emotion_recognizer': emotion_recognizer.state_dict(), 'opt': opt}\n",
    "#         torch.save(state, model_file_name)\n",
    "\n",
    "        if test_acc['emotion_acc'] > best_acc:\n",
    "#             model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'model.pth.tar')\n",
    "#             torch.save(state, model_file_name)\n",
    "\n",
    "            best_acc = test_acc['emotion_acc']\n",
    "\n",
    "        if test_uar['emotion_uar'] > best_uar:\n",
    "            best_uar = test_uar['emotion_uar']\n",
    "\n",
    "        if es.step(test_loss['emotion_loss']):\n",
    "            break\n",
    "\n",
    "    return best_acc, best_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "extraordinary-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(train_loader, model, optimizer, criterion, device, verbose = False):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    groundtruth = []\n",
    "    prediction = []\n",
    "\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = train_data # UPDATE\n",
    "\n",
    "        visual_features = visual_features.to(device)\n",
    "        acoustic_features = acoustic_features.to(device)\n",
    "        lexical_features = lexical_features.to(device)\n",
    "\n",
    "        labels = a_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emotion_output, stress_output = model(visual_features, acoustic_features, lexical_features)\n",
    "\n",
    "        emotion_loss = criterion(emotion_output, labels)\n",
    "#         stress_loss = criterion(stress_output, stress_labels)\n",
    "\n",
    "        emotion_loss.backward()\n",
    "#         stress_loss.backward()\n",
    "        \n",
    "        optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "        running_loss += emotion_loss.item()\n",
    "\n",
    "        groundtruth.append(labels.tolist())\n",
    "        predictions = emotion_output.argmax(dim=1, keepdim=True)\n",
    "        prediction.append(predictions.view_as(labels).tolist())\n",
    "\n",
    "        if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "            print('.', flush=True, end='')\n",
    "            \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "    prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "    train_acc = accuracy_score(prediction, groundtruth)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-wesley",
   "metadata": {},
   "source": [
    "When we train baseline, the loss from the confound is NOT included in backpropagation, only the loss from the emotion task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mexican-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, criterion, device, verbose = False, emotion_dimension = 'arousal', baseline = False):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    emotion_running_loss = 0.\n",
    "    confound_running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    emotion_groundtruth = []\n",
    "    emotion_prediction = []\n",
    "    \n",
    "    confound_groundtruth = []\n",
    "    confound_prediction = []\n",
    "\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, s_labels, _ = train_data # UPDATE\n",
    "\n",
    "        visual_features = visual_features.to(device)\n",
    "        acoustic_features = acoustic_features.to(device)\n",
    "        lexical_features = lexical_features.to(device)\n",
    "\n",
    "        if emotion_dimension == 'arousal':\n",
    "            emotion_labels = a_labels.to(device)\n",
    "        elif emotion_dimension == 'valence':\n",
    "            emotion_labels = v_labels.to(device)\n",
    "        else:\n",
    "            print(\"Invalid emotion dimension\")\n",
    "            return\n",
    "        confound_labels = s_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "        \n",
    "        emotion_loss = criterion(emotion_predictions, emotion_labels)\n",
    "        confound_loss = criterion(confound_predictions, confound_labels)\n",
    "        loss = emotion_loss + confound_loss\n",
    "        if baseline == True:\n",
    "            loss = emotion_loss\n",
    "        else:\n",
    "            loss = emotion_loss + confound_loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "        emotion_running_loss += emotion_loss.item()\n",
    "        confound_running_loss += confound_loss.item()\n",
    "        running_loss += emotion_running_loss + confound_running_loss\n",
    "\n",
    "        emotion_groundtruth.append(emotion_labels.tolist())\n",
    "        emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "        emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\n",
    "        \n",
    "        confound_groundtruth.append(confound_labels.tolist())\n",
    "        confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\n",
    "        confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\n",
    "\n",
    "        if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "            print('.', flush=True, end='')\n",
    "        \n",
    "    emotion_loss = emotion_running_loss / len(train_loader)\n",
    "    confound_loss = confound_running_loss / len(train_loader)\n",
    "    loss = running_loss / len(train_loader)\n",
    "    train_loss = {'emotion_loss': emotion_loss,\n",
    "                  'confound_loss': confound_loss,\n",
    "                  'loss': loss\n",
    "                 }\n",
    "\n",
    "    emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\n",
    "    emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\n",
    "    \n",
    "    confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\n",
    "    confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\n",
    "\n",
    "    emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\n",
    "    confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\n",
    "    avg_acc = (emotion_acc + confound_acc) / 2\n",
    "    \n",
    "    train_acc = {'emotion_acc': emotion_acc,\n",
    "                  'confound_acc': confound_acc\n",
    "                }\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "egyptian-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(test_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        groundtruth = []\n",
    "        prediction = []\n",
    "\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, _ = test_data # UPDATE\n",
    "\n",
    "            visual_features = visual_features.to(device)\n",
    "            acoustic_features = acoustic_features.to(device)\n",
    "            lexical_features = lexical_features.to(device)\n",
    "\n",
    "            labels = a_labels.to(device)\n",
    "\n",
    "            emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "            loss = criterion(emotion_predictions, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            groundtruth.append(labels.tolist())\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "            prediction.append(emotion_predictions.view_as(labels).tolist())\n",
    "\n",
    "        test_loss = running_loss / len(test_loader)\n",
    "\n",
    "        groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "        prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "        test_acc = accuracy_score(prediction, groundtruth)\n",
    "        test_uar = recall_score(prediction, groundtruth, average='macro')\n",
    "\n",
    "        return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sticky-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device, emotion_dimension = 'arousal'):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.\n",
    "    emotion_running_loss = 0.\n",
    "    confound_running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotion_groundtruth = []\n",
    "        emotion_prediction = []\n",
    "\n",
    "        confound_groundtruth = []\n",
    "        confound_prediction = []\n",
    "\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, s_labels, _ = test_data # UPDATE\n",
    "\n",
    "            visual_features = visual_features.to(device)\n",
    "            acoustic_features = acoustic_features.to(device)\n",
    "            lexical_features = lexical_features.to(device)\n",
    "            \n",
    "            if emotion_dimension == 'arousal':\n",
    "                emotion_labels = a_labels.to(device)\n",
    "            elif emotion_dimension == 'valence':\n",
    "                emotion_labels = v_labels.to(device)\n",
    "            else:\n",
    "                print(\"Invalid emotion dimension\")\n",
    "                return\n",
    "            confound_labels = s_labels.to(device)\n",
    "\n",
    "            emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "            \n",
    "            emotion_loss = criterion(emotion_predictions, emotion_labels)\n",
    "            confound_loss = criterion(confound_predictions, confound_labels)\n",
    "            loss = emotion_loss + confound_loss\n",
    "\n",
    "            emotion_running_loss += emotion_loss.item()\n",
    "            confound_running_loss += confound_loss.item()\n",
    "            running_loss += emotion_running_loss + confound_running_loss\n",
    "\n",
    "            emotion_groundtruth.append(emotion_labels.tolist())\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "            emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\n",
    "\n",
    "            confound_groundtruth.append(confound_labels.tolist())\n",
    "            confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\n",
    "            confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\n",
    "\n",
    "        emotion_loss = emotion_running_loss / len(train_loader)\n",
    "        confound_loss = confound_running_loss / len(train_loader)\n",
    "        loss = running_loss / len(train_loader)\n",
    "        test_loss = {'emotion_loss': emotion_loss,\n",
    "                     'confound_loss': confound_loss,\n",
    "                     'loss': loss\n",
    "                    }\n",
    "\n",
    "        emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\n",
    "        emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\n",
    "\n",
    "        confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\n",
    "        confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\n",
    "\n",
    "        emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\n",
    "        confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\n",
    "        avg_acc = (emotion_acc + confound_acc) / 2\n",
    "        test_acc = {'emotion_acc': emotion_acc,\n",
    "                    'confound_acc': confound_acc\n",
    "                   }\n",
    "        \n",
    "        emotion_uar = recall_score(emotion_prediction, emotion_groundtruth, average='macro')\n",
    "        confound_uar = recall_score(confound_prediction, confound_groundtruth, average='macro')\n",
    "\n",
    "        test_uar = {'emotion_uar': emotion_uar,\n",
    "                    'confound_uar': confound_uar\n",
    "                   }\n",
    "\n",
    "        return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "taken-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-source",
   "metadata": {},
   "source": [
    "## Train + Test for Valence and Arousal with adversarial learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-withdrawal",
   "metadata": {},
   "source": [
    "### Audio and Lexical w/ Adversarial for Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "corrected-designation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "Train loss: {'emotion_loss': 1.0262766043847638, 'confound_loss': 0.6968713596405223, 'loss': 897.7760060264327}\n",
      "Train acc: {'emotion_acc': 0.5085158150851582, 'confound_acc': 0.5141119221411192}\n",
      "Epoch 1/3\n",
      "Train loss: {'emotion_loss': 0.9685588047662134, 'confound_loss': 0.69297020433014, 'loss': 861.1841793767093}\n",
      "Train acc: {'emotion_acc': 0.57007299270073, 'confound_acc': 0.5110705596107056}\n",
      "Epoch 2/3\n",
      "Train loss: {'emotion_loss': 0.9001411217543865, 'confound_loss': 0.6904657095206851, 'loss': 821.7689868371194}\n",
      "Train acc: {'emotion_acc': 0.6491484184914842, 'confound_acc': 0.5115571776155717}\n"
     ]
    }
   ],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'valence')\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "endangered-attraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: {'emotion_loss': 0.19944935961688076, 'confound_loss': 0.1478692471749124, 'loss': 40.20029201088944}\n",
      "Test acc: {'emotion_acc': 0.6421110500274876, 'confound_acc': 0.5838372732270478}\n",
      "Test uar: {'emotion_uar': 0.4130423093725846, 'confound_uar': 0.6667355371900827}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandon/src/stressed_emotion/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'valence')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "right-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fed487ec310>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss for Acoustic and Lexical modalities with Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Valence loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-school",
   "metadata": {},
   "source": [
    "### Audio and Lexical w/ Adversarial for Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-emergency",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'arousal')\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'arousal')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Acoustic and Lexical modalities with Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Arousal loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-allergy",
   "metadata": {},
   "source": [
    "### Acoustic w/ Adversarial for Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "anonymous-microphone",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30\n",
      "Train loss: {'emotion_loss': 1.022733929730111, 'confound_loss': 0.6960138081344649, 'loss': 894.0060584385687}\n",
      "Train acc: {'emotion_acc': 0.5222627737226277, 'confound_acc': 0.505352798053528}\n",
      "Epoch 1/30\n",
      "Train loss: {'emotion_loss': 1.0005389876982582, 'confound_loss': 0.709725619464069, 'loss': 879.2162220832323}\n",
      "Train acc: {'emotion_acc': 0.545985401459854, 'confound_acc': 0.49330900243309}\n",
      "Epoch 2/30\n",
      "Train loss: {'emotion_loss': 0.9972552922101336, 'confound_loss': 0.6954831711744984, 'loss': 869.1035517824647}\n",
      "Train acc: {'emotion_acc': 0.5479318734793187, 'confound_acc': 0.5052311435523115}\n",
      "Epoch 3/30\n",
      "Train loss: {'emotion_loss': 0.9924549764812225, 'confound_loss': 0.6980505176323397, 'loss': 867.7392715406442}\n",
      "Train acc: {'emotion_acc': 0.5525547445255474, 'confound_acc': 0.5030413625304136}\n",
      "Epoch 4/30\n",
      "Train loss: {'emotion_loss': 0.9938927118300463, 'confound_loss': 0.696831961434175, 'loss': 870.4618173838821}\n",
      "Train acc: {'emotion_acc': 0.5502433090024331, 'confound_acc': 0.5035279805352798}\n",
      "Epoch 5/30\n",
      "Train loss: {'emotion_loss': 1.0011690453216724, 'confound_loss': 0.6964559573027874, 'loss': 873.6830756786037}\n",
      "Train acc: {'emotion_acc': 0.5407542579075426, 'confound_acc': 0.4962287104622871}\n",
      "Epoch 6/30\n",
      "Train loss: {'emotion_loss': 0.999815285843634, 'confound_loss': 0.6953958607485322, 'loss': 872.3842481513896}\n",
      "Train acc: {'emotion_acc': 0.540632603406326, 'confound_acc': 0.497323600973236}\n",
      "Epoch 7/30\n",
      "Train loss: {'emotion_loss': 0.9950135496919721, 'confound_loss': 0.6932245991573259, 'loss': 868.2251237810585}\n",
      "Train acc: {'emotion_acc': 0.5454987834549878, 'confound_acc': 0.5025547445255475}\n",
      "Epoch 8/30\n",
      "Train loss: {'emotion_loss': 0.9945230802905235, 'confound_loss': 0.6936234299542839, 'loss': 866.1645528463654}\n",
      "Train acc: {'emotion_acc': 0.5472019464720195, 'confound_acc': 0.49805352798053526}\n",
      "Epoch 9/30\n",
      "Train loss: {'emotion_loss': 1.0002643152317647, 'confound_loss': 0.6926137994003667, 'loss': 872.742813413825}\n",
      "Train acc: {'emotion_acc': 0.5397810218978102, 'confound_acc': 0.5128953771289537}\n",
      "Epoch 10/30\n",
      "Train loss: {'emotion_loss': 0.9927238023002788, 'confound_loss': 0.6929094765900637, 'loss': 866.8568892551419}\n",
      "Train acc: {'emotion_acc': 0.5474452554744526, 'confound_acc': 0.5153284671532846}\n",
      "Epoch 11/30\n",
      "Train loss: {'emotion_loss': 0.9983174098140999, 'confound_loss': 0.6930310941855731, 'loss': 869.9958411829133}\n",
      "Train acc: {'emotion_acc': 0.5395377128953771, 'confound_acc': 0.5087591240875913}\n",
      "Epoch 12/30\n",
      "Train loss: {'emotion_loss': 0.993501187530473, 'confound_loss': 0.6931673437936761, 'loss': 868.1017523229934}\n",
      "Train acc: {'emotion_acc': 0.5442822384428224, 'confound_acc': 0.5080291970802919}\n",
      "Epoch 13/30\n",
      "Train loss: {'emotion_loss': 0.9920798227017028, 'confound_loss': 0.6910708391828759, 'loss': 865.4492634598035}\n",
      "Train acc: {'emotion_acc': 0.5441605839416058, 'confound_acc': 0.5338199513381995}\n",
      "Epoch 14/30\n",
      "Train loss: {'emotion_loss': 0.9937218700169589, 'confound_loss': 0.6920015639716085, 'loss': 867.9747173543223}\n",
      "Train acc: {'emotion_acc': 0.5420924574209246, 'confound_acc': 0.517396593673966}\n",
      "Epoch 15/30\n",
      "Train loss: {'emotion_loss': 0.9855094582770121, 'confound_loss': 0.6929099978987808, 'loss': 864.1901823025966}\n",
      "Train acc: {'emotion_acc': 0.5548661800486618, 'confound_acc': 0.5194647201946472}\n",
      "Epoch 16/30\n",
      "Train loss: {'emotion_loss': 0.993773777777119, 'confound_loss': 0.693016210128825, 'loss': 868.7476571713672}\n",
      "Train acc: {'emotion_acc': 0.5431873479318735, 'confound_acc': 0.49878345498783455}\n",
      "Epoch 17/30\n",
      "Train loss: {'emotion_loss': 0.9921045914466279, 'confound_loss': 0.6923873000107851, 'loss': 866.8715379036934}\n",
      "Train acc: {'emotion_acc': 0.5448905109489051, 'confound_acc': 0.5051094890510949}\n",
      "Epoch 18/30\n",
      "Train loss: {'emotion_loss': 0.9958581858919753, 'confound_loss': 0.691785853602543, 'loss': 868.0284429529994}\n",
      "Train acc: {'emotion_acc': 0.5375912408759124, 'confound_acc': 0.5169099756690998}\n",
      "Epoch 19/30\n",
      "Train loss: {'emotion_loss': 0.9892499777361576, 'confound_loss': 0.6921848697073265, 'loss': 865.249350620325}\n",
      "Train acc: {'emotion_acc': 0.5481751824817518, 'confound_acc': 0.5152068126520681}\n",
      "Epoch 20/30\n",
      "Train loss: {'emotion_loss': 0.9875020774421989, 'confound_loss': 0.6920049763955031, 'loss': 864.0576289874803}\n",
      "Train acc: {'emotion_acc': 0.5514598540145985, 'confound_acc': 0.5193430656934307}\n",
      "Epoch 21/30\n",
      "Train loss: {'emotion_loss': 0.9835651309341772, 'confound_loss': 0.6910486136081154, 'loss': 859.8488294808085}\n",
      "Train acc: {'emotion_acc': 0.5541362530413625, 'confound_acc': 0.5231143552311436}\n",
      "Epoch 22/30\n",
      "Train loss: {'emotion_loss': 0.9896822592627678, 'confound_loss': 0.6912992499797724, 'loss': 865.8818124467181}\n",
      "Train acc: {'emotion_acc': 0.5453771289537713, 'confound_acc': 0.5351581508515815}\n",
      "Epoch 23/30\n",
      "Train loss: {'emotion_loss': 0.9852932478783195, 'confound_loss': 0.6917101027668682, 'loss': 864.5915068657018}\n",
      "Train acc: {'emotion_acc': 0.5498783454987834, 'confound_acc': 0.5277372262773723}\n",
      "Epoch 24/30\n",
      "Train loss: {'emotion_loss': 0.987088678237993, 'confound_loss': 0.6925727477449387, 'loss': 865.8136186212997}\n",
      "Train acc: {'emotion_acc': 0.55, 'confound_acc': 0.5079075425790754}\n",
      "Epoch 25/30\n",
      "Train loss: {'emotion_loss': 0.9899845557569994, 'confound_loss': 0.6923217894618149, 'loss': 864.5947087311444}\n",
      "Train acc: {'emotion_acc': 0.5465936739659367, 'confound_acc': 0.5121654501216545}\n",
      "Epoch 26/30\n",
      "Train loss: {'emotion_loss': 0.967767739864175, 'confound_loss': 0.6903933327253691, 'loss': 853.2326685587371}\n",
      "Train acc: {'emotion_acc': 0.5704379562043795, 'confound_acc': 0.5362530413625304}\n",
      "Epoch 27/30\n",
      "Train loss: {'emotion_loss': 0.974544142296806, 'confound_loss': 0.690948723007269, 'loss': 854.1618814439973}\n",
      "Train acc: {'emotion_acc': 0.5644768856447688, 'confound_acc': 0.5204379562043796}\n",
      "Epoch 28/30\n",
      "Train loss: {'emotion_loss': 0.975706878281289, 'confound_loss': 0.6930353012414294, 'loss': 860.9258600489059}\n",
      "Train acc: {'emotion_acc': 0.5638686131386861, 'confound_acc': 0.5065693430656935}\n",
      "Epoch 29/30\n",
      "Train loss: {'emotion_loss': 0.9778337071030057, 'confound_loss': 0.6914019767296453, 'loss': 860.4489858479815}\n",
      "Train acc: {'emotion_acc': 0.5642335766423358, 'confound_acc': 0.5156934306569343}\n"
     ]
    }
   ],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = False, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'valence')\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'valence')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Acoustic modalities with Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Valence loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-canon",
   "metadata": {},
   "source": [
    "### Acoustic w/ Adversarial for Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-small",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = False, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'arousal')\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'arousal')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Acoustic modality with Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Arousal loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-transfer",
   "metadata": {},
   "source": [
    "### Lexical w/ Adversarial for Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-soldier",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = False, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'valence')\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'valence')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Lexical modality with Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Valence loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-necklace",
   "metadata": {},
   "source": [
    "### Lexical w/ Adversarial for Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-radio",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = False, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'arousal')\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'arousal')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Lexical modality with Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Arousal loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-adjustment",
   "metadata": {},
   "source": [
    "## Train + Test for Valence and Arousal without adversarial learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-interim",
   "metadata": {},
   "source": [
    "### Audio and Lexical w/o Adversarial for Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ordinary-victor",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "Train loss: {'emotion_loss': 1.0262766043847638, 'confound_loss': 0.6968713596405223, 'loss': 897.7760060264327}\n",
      "Train acc: {'emotion_acc': 0.5085158150851582, 'confound_acc': 0.5141119221411192}\n",
      "Epoch 1/3\n",
      "Train loss: {'emotion_loss': 0.9685588047662134, 'confound_loss': 0.69297020433014, 'loss': 861.1841793767093}\n",
      "Train acc: {'emotion_acc': 0.57007299270073, 'confound_acc': 0.5110705596107056}\n",
      "Epoch 2/3\n",
      "Train loss: {'emotion_loss': 0.9001411217543865, 'confound_loss': 0.6904657095206851, 'loss': 821.7689868371194}\n",
      "Train acc: {'emotion_acc': 0.6491484184914842, 'confound_acc': 0.5115571776155717}\n"
     ]
    }
   ],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'valence', baseline=True)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "miniature-color",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: {'emotion_loss': 0.19944935961688076, 'confound_loss': 0.1478692471749124, 'loss': 40.20029201088944}\n",
      "Test acc: {'emotion_acc': 0.6421110500274876, 'confound_acc': 0.5838372732270478}\n",
      "Test uar: {'emotion_uar': 0.4130423093725846, 'confound_uar': 0.6667355371900827}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandon/src/stressed_emotion/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'valence')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "lesser-trail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fed487ec310>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss for Acoustic and Lexical modalities without Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Valence loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-commission",
   "metadata": {},
   "source": [
    "### Audio and Lexical w/ Adversarial for Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-anatomy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'arousal', baseline=True)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'arousal')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Acoustic and Lexical modalities without Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Arousal loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-national",
   "metadata": {},
   "source": [
    "### Acoustic w/ Adversarial for Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "large-complexity",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30\n",
      "Train loss: {'emotion_loss': 1.022733929730111, 'confound_loss': 0.6960138081344649, 'loss': 894.0060584385687}\n",
      "Train acc: {'emotion_acc': 0.5222627737226277, 'confound_acc': 0.505352798053528}\n",
      "Epoch 1/30\n",
      "Train loss: {'emotion_loss': 1.0005389876982582, 'confound_loss': 0.709725619464069, 'loss': 879.2162220832323}\n",
      "Train acc: {'emotion_acc': 0.545985401459854, 'confound_acc': 0.49330900243309}\n",
      "Epoch 2/30\n",
      "Train loss: {'emotion_loss': 0.9972552922101336, 'confound_loss': 0.6954831711744984, 'loss': 869.1035517824647}\n",
      "Train acc: {'emotion_acc': 0.5479318734793187, 'confound_acc': 0.5052311435523115}\n",
      "Epoch 3/30\n",
      "Train loss: {'emotion_loss': 0.9924549764812225, 'confound_loss': 0.6980505176323397, 'loss': 867.7392715406442}\n",
      "Train acc: {'emotion_acc': 0.5525547445255474, 'confound_acc': 0.5030413625304136}\n",
      "Epoch 4/30\n",
      "Train loss: {'emotion_loss': 0.9938927118300463, 'confound_loss': 0.696831961434175, 'loss': 870.4618173838821}\n",
      "Train acc: {'emotion_acc': 0.5502433090024331, 'confound_acc': 0.5035279805352798}\n",
      "Epoch 5/30\n",
      "Train loss: {'emotion_loss': 1.0011690453216724, 'confound_loss': 0.6964559573027874, 'loss': 873.6830756786037}\n",
      "Train acc: {'emotion_acc': 0.5407542579075426, 'confound_acc': 0.4962287104622871}\n",
      "Epoch 6/30\n",
      "Train loss: {'emotion_loss': 0.999815285843634, 'confound_loss': 0.6953958607485322, 'loss': 872.3842481513896}\n",
      "Train acc: {'emotion_acc': 0.540632603406326, 'confound_acc': 0.497323600973236}\n",
      "Epoch 7/30\n",
      "Train loss: {'emotion_loss': 0.9950135496919721, 'confound_loss': 0.6932245991573259, 'loss': 868.2251237810585}\n",
      "Train acc: {'emotion_acc': 0.5454987834549878, 'confound_acc': 0.5025547445255475}\n",
      "Epoch 8/30\n",
      "Train loss: {'emotion_loss': 0.9945230802905235, 'confound_loss': 0.6936234299542839, 'loss': 866.1645528463654}\n",
      "Train acc: {'emotion_acc': 0.5472019464720195, 'confound_acc': 0.49805352798053526}\n",
      "Epoch 9/30\n",
      "Train loss: {'emotion_loss': 1.0002643152317647, 'confound_loss': 0.6926137994003667, 'loss': 872.742813413825}\n",
      "Train acc: {'emotion_acc': 0.5397810218978102, 'confound_acc': 0.5128953771289537}\n",
      "Epoch 10/30\n",
      "Train loss: {'emotion_loss': 0.9927238023002788, 'confound_loss': 0.6929094765900637, 'loss': 866.8568892551419}\n",
      "Train acc: {'emotion_acc': 0.5474452554744526, 'confound_acc': 0.5153284671532846}\n",
      "Epoch 11/30\n",
      "Train loss: {'emotion_loss': 0.9983174098140999, 'confound_loss': 0.6930310941855731, 'loss': 869.9958411829133}\n",
      "Train acc: {'emotion_acc': 0.5395377128953771, 'confound_acc': 0.5087591240875913}\n",
      "Epoch 12/30\n",
      "Train loss: {'emotion_loss': 0.993501187530473, 'confound_loss': 0.6931673437936761, 'loss': 868.1017523229934}\n",
      "Train acc: {'emotion_acc': 0.5442822384428224, 'confound_acc': 0.5080291970802919}\n",
      "Epoch 13/30\n",
      "Train loss: {'emotion_loss': 0.9920798227017028, 'confound_loss': 0.6910708391828759, 'loss': 865.4492634598035}\n",
      "Train acc: {'emotion_acc': 0.5441605839416058, 'confound_acc': 0.5338199513381995}\n",
      "Epoch 14/30\n",
      "Train loss: {'emotion_loss': 0.9937218700169589, 'confound_loss': 0.6920015639716085, 'loss': 867.9747173543223}\n",
      "Train acc: {'emotion_acc': 0.5420924574209246, 'confound_acc': 0.517396593673966}\n",
      "Epoch 15/30\n",
      "Train loss: {'emotion_loss': 0.9855094582770121, 'confound_loss': 0.6929099978987808, 'loss': 864.1901823025966}\n",
      "Train acc: {'emotion_acc': 0.5548661800486618, 'confound_acc': 0.5194647201946472}\n",
      "Epoch 16/30\n",
      "Train loss: {'emotion_loss': 0.993773777777119, 'confound_loss': 0.693016210128825, 'loss': 868.7476571713672}\n",
      "Train acc: {'emotion_acc': 0.5431873479318735, 'confound_acc': 0.49878345498783455}\n",
      "Epoch 17/30\n",
      "Train loss: {'emotion_loss': 0.9921045914466279, 'confound_loss': 0.6923873000107851, 'loss': 866.8715379036934}\n",
      "Train acc: {'emotion_acc': 0.5448905109489051, 'confound_acc': 0.5051094890510949}\n",
      "Epoch 18/30\n",
      "Train loss: {'emotion_loss': 0.9958581858919753, 'confound_loss': 0.691785853602543, 'loss': 868.0284429529994}\n",
      "Train acc: {'emotion_acc': 0.5375912408759124, 'confound_acc': 0.5169099756690998}\n",
      "Epoch 19/30\n",
      "Train loss: {'emotion_loss': 0.9892499777361576, 'confound_loss': 0.6921848697073265, 'loss': 865.249350620325}\n",
      "Train acc: {'emotion_acc': 0.5481751824817518, 'confound_acc': 0.5152068126520681}\n",
      "Epoch 20/30\n",
      "Train loss: {'emotion_loss': 0.9875020774421989, 'confound_loss': 0.6920049763955031, 'loss': 864.0576289874803}\n",
      "Train acc: {'emotion_acc': 0.5514598540145985, 'confound_acc': 0.5193430656934307}\n",
      "Epoch 21/30\n",
      "Train loss: {'emotion_loss': 0.9835651309341772, 'confound_loss': 0.6910486136081154, 'loss': 859.8488294808085}\n",
      "Train acc: {'emotion_acc': 0.5541362530413625, 'confound_acc': 0.5231143552311436}\n",
      "Epoch 22/30\n",
      "Train loss: {'emotion_loss': 0.9896822592627678, 'confound_loss': 0.6912992499797724, 'loss': 865.8818124467181}\n",
      "Train acc: {'emotion_acc': 0.5453771289537713, 'confound_acc': 0.5351581508515815}\n",
      "Epoch 23/30\n",
      "Train loss: {'emotion_loss': 0.9852932478783195, 'confound_loss': 0.6917101027668682, 'loss': 864.5915068657018}\n",
      "Train acc: {'emotion_acc': 0.5498783454987834, 'confound_acc': 0.5277372262773723}\n",
      "Epoch 24/30\n",
      "Train loss: {'emotion_loss': 0.987088678237993, 'confound_loss': 0.6925727477449387, 'loss': 865.8136186212997}\n",
      "Train acc: {'emotion_acc': 0.55, 'confound_acc': 0.5079075425790754}\n",
      "Epoch 25/30\n",
      "Train loss: {'emotion_loss': 0.9899845557569994, 'confound_loss': 0.6923217894618149, 'loss': 864.5947087311444}\n",
      "Train acc: {'emotion_acc': 0.5465936739659367, 'confound_acc': 0.5121654501216545}\n",
      "Epoch 26/30\n",
      "Train loss: {'emotion_loss': 0.967767739864175, 'confound_loss': 0.6903933327253691, 'loss': 853.2326685587371}\n",
      "Train acc: {'emotion_acc': 0.5704379562043795, 'confound_acc': 0.5362530413625304}\n",
      "Epoch 27/30\n",
      "Train loss: {'emotion_loss': 0.974544142296806, 'confound_loss': 0.690948723007269, 'loss': 854.1618814439973}\n",
      "Train acc: {'emotion_acc': 0.5644768856447688, 'confound_acc': 0.5204379562043796}\n",
      "Epoch 28/30\n",
      "Train loss: {'emotion_loss': 0.975706878281289, 'confound_loss': 0.6930353012414294, 'loss': 860.9258600489059}\n",
      "Train acc: {'emotion_acc': 0.5638686131386861, 'confound_acc': 0.5065693430656935}\n",
      "Epoch 29/30\n",
      "Train loss: {'emotion_loss': 0.9778337071030057, 'confound_loss': 0.6914019767296453, 'loss': 860.4489858479815}\n",
      "Train acc: {'emotion_acc': 0.5642335766423358, 'confound_acc': 0.5156934306569343}\n"
     ]
    }
   ],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = False, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'valence', baseline=True)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'valence')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Acoustic modalities with Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Valence loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-pressing",
   "metadata": {},
   "source": [
    "### Acoustic w/ Adversarial for Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-negative",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = False, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'arousal', baseline=True)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'arousal')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Acoustic modality without Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Arousal loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-basement",
   "metadata": {},
   "source": [
    "### Lexical w/ Adversarial for Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-vancouver",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = False, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'valence', baseline=True)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'valence')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Lexical modality without Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Valence loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-builder",
   "metadata": {},
   "source": [
    "### Lexical w/ Adversarial for Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-indication",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = False, lexical_modality = True, visual_modality = False, grl_lambda = .8)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, emotion_dimension = 'arousal', baseline=True)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, emotion_dimension = 'arousal')\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "print(f'Test uar: {test_uar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss for Lexical modality without Adversarial Learning')\n",
    "plt.plot(range(epochs), emotion_loss, label = 'Arousal loss')\n",
    "plt.plot(range(epochs), confound_loss, label = 'Confound loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-summary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-trial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-mitchell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-sight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-anatomy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-handle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-powell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-thumb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-louisville",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-exploration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-satellite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-linux",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-greensboro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-worry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vertical-force",
   "metadata": {},
   "source": [
    "## Train all folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "uar = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 0, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 1, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 2, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 3, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 4, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(5),acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(5),uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([acc,uar]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(uar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
