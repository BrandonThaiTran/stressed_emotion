{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "upper-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sampler\n",
    "import datasets\n",
    "from earlystopping import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from torch.autograd  import  Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-dance",
   "metadata": {},
   "source": [
    "# Acoustic Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-floating",
   "metadata": {},
   "source": [
    "Inputs for acoustic branch will be N x 40 where N [1,33]  \n",
    "Time step: (2, 10) (seconds?)  \n",
    "N: relative duration after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "decreased-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticNet(nn.Module):\n",
    "    def __init__(self, num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2):\n",
    "        super(AcousticNet, self).__init__()\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.convs = [self.conv1, self.conv2, self.conv3, self.conv4]\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=conv_width,hidden_size=32,num_layers=num_gru_layers) # 19 is hardcoded\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "#         print(x.shape)\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.relu(self.max_pool(self.convs[i](x)))\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = F.adaptive_avg_pool1d(x,1)[:, :, -1]\n",
    "#         x = self.mean_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "color-teaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = AcousticNet(num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "test_vec = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "output = net(test_vec)\n",
    "print(f'Shape of output: {output.shape}')\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-medicaid",
   "metadata": {},
   "source": [
    "# Lexical Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "center-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement GRU (or transformer)\n",
    "class LexicalNet(nn.Module):\n",
    "    def __init__(self, num_gru_layers = 2):\n",
    "        super(LexicalNet, self).__init__()\n",
    "        # implement GRU (or transformer)\n",
    "        self.gru = nn.GRU(input_size=768,hidden_size=32,num_layers=num_gru_layers)\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2) \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "#         x = self.mean_pool(x)\n",
    "        x = self.flatten(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sonic-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dummy input\n",
    "net = LexicalNet(num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "test_vec = torch.randn(batch_size, 1, 768)\n",
    "output = net(test_vec)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-confidence",
   "metadata": {},
   "source": [
    "# Master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "boolean-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRL(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self,x):\n",
    "#         return x\n",
    "#     @staticmethod\n",
    "#     def backward(self,grad_output):\n",
    "#         grad_input = grad_output.neg()\n",
    "#         return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "early-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "    \n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "liquid-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterNet(nn.Module):\n",
    "    def __init__(self, acoustic_modality = True, lexical_modality = True, visual_modality = False,\n",
    "                 num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2,\n",
    "                 num_dense_layers = 1, dense_layer_width = 32, grl_lambda = .3):\n",
    "        super(MasterNet, self).__init__()\n",
    "        \n",
    "        self.acoustic_modality = acoustic_modality\n",
    "        self.lexical_modality = lexical_modality\n",
    "        self.visual_modality = visual_modality\n",
    "        \n",
    "        self.acoustic_model = AcousticNet(num_conv_layers = num_conv_layers, kernel_size = kernel_size, \n",
    "                                     conv_width = conv_width, num_gru_layers = num_gru_layers)\n",
    "        self.lexical_model = LexicalNet(num_gru_layers = 2)\n",
    "        \n",
    "        # emotion classifier\n",
    "#         self.dense1_emo = nn.Linear()\n",
    "#         self.dense2_emo = nn.Linear()\n",
    "        \n",
    "        width = 0 # width of the FC layers\n",
    "        if self.acoustic_modality:\n",
    "            width += 32\n",
    "        if self.visual_modality:\n",
    "            width += 0 # to implement\n",
    "        if self.lexical_modality:\n",
    "            width += 32\n",
    "            \n",
    "        self.fc_1 = nn.Linear(width, dense_layer_width)\n",
    "        self.fc_2 = nn.Linear(dense_layer_width, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "#         # To implement   \n",
    "#         if num_dense_layers == 2:\n",
    "#             self.fc = nn.Sequential()\n",
    "#             self.linear_1 = nn.Linear(width, dense_layer_width)\n",
    "#         else:\n",
    "#             self.fc = \n",
    "        \n",
    "        # confound classifier -- to implement\n",
    "        \n",
    "        self.grl = GradientReversal(lambda_ = grl_lambda)\n",
    "        self.dense_con = nn.Linear(width, 3)\n",
    "#         self.dense2_con = None\n",
    "        \n",
    "        \n",
    "    def forward_a(self, x_a):\n",
    "        x = x_a\n",
    "        x = self.acoustic_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_l(self, x_l):\n",
    "        x = torch.unsqueeze(x_l, dim = 1)\n",
    "        x = self.lexical_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_v(self, x_v):\n",
    "        x = x_v\n",
    "        return x\n",
    "    \n",
    "    def encoder(self, x_v, x_a, x_l):\n",
    "#         print('x_a before encoding', x_a.shape)\n",
    "#         print('x_l before encoding', x_l.shape)\n",
    "        if self.visual_modality:\n",
    "            x_v = self.forward_v(x_v)\n",
    "        if self.acoustic_modality:\n",
    "            x_a = self.forward_a(x_a)\n",
    "        if self.lexical_modality:\n",
    "            x_l = self.forward_l(x_l)\n",
    "#         print('x_a after encoding', x_a.shape)\n",
    "#         print('x_l after encoding', x_l.shape)\n",
    "        \n",
    "        if self.visual_modality:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = torch.cat((x_v, x_a), 1)\n",
    "            else:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_l), 1)\n",
    "                else:\n",
    "                    x = x_v\n",
    "        else:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = x_a\n",
    "            else:\n",
    "                x = x_l\n",
    "#         print('x after concat', x.shape)\n",
    "        return x\n",
    "\n",
    "    def confound_model(self, x):\n",
    "#         x = self.grl.apply(x)\n",
    "        x = self.grl(x)\n",
    "        x = self.dense_con(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def recognizer(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_v, x_a, x_l):\n",
    "        x = self.encoder(x_v, x_a, x_l)\n",
    "        emotion_output = self.recognizer(x)\n",
    "        confound_output = self.confound_model(x)\n",
    "        \n",
    "        return emotion_output, confound_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "immune-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of emotion output: torch.Size([8, 3])\n",
      "Shape of stress output: torch.Size([8, 3])\n",
      "tensor([[0.3326, 0.3622, 0.3052],\n",
      "        [0.3367, 0.3617, 0.3016],\n",
      "        [0.3361, 0.3633, 0.3007],\n",
      "        [0.3386, 0.3576, 0.3038],\n",
      "        [0.3372, 0.3577, 0.3051],\n",
      "        [0.3290, 0.3629, 0.3081],\n",
      "        [0.3260, 0.3593, 0.3148],\n",
      "        [0.3260, 0.3585, 0.3155]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2877, 0.3320, 0.3803],\n",
      "        [0.2769, 0.3426, 0.3806],\n",
      "        [0.2895, 0.3631, 0.3474],\n",
      "        [0.2853, 0.3449, 0.3698],\n",
      "        [0.2615, 0.3569, 0.3816],\n",
      "        [0.2609, 0.3809, 0.3582],\n",
      "        [0.2673, 0.3849, 0.3478],\n",
      "        [0.2780, 0.3669, 0.3552]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = MasterNet()\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "acoustic_features = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "# lexical_features = torch.randn(batch_size, 1, 300)\n",
    "lexical_features = torch.randn(batch_size, 768)\n",
    "visual_features = None\n",
    "emotion_output, stress_output = net(visual_features, acoustic_features, lexical_features)\n",
    "print(f'Shape of emotion output: {emotion_output.shape}')\n",
    "print(f'Shape of stress output: {stress_output.shape}')\n",
    "print(emotion_output)\n",
    "print(stress_output)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "veterinary-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use specific GPU\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():  \n",
    "        dev = \"cuda:0\" \n",
    "    else:  \n",
    "        dev = \"cpu\"  \n",
    "    return torch.device(dev)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "prescription-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_folder(model, folder = 0, epochs = 1, verbose = False, learning_rate = 1e-4, patience = 5):\n",
    "    # Use specific GPU\n",
    "    device = get_device()\n",
    "\n",
    "    # Dataloaders    \n",
    "    train_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/train.csv'\n",
    "    train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "    test_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/test.csv'\n",
    "    test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "    # Model, optimizer and loss function\n",
    "    emotion_recognizer = model\n",
    "    init_weights(emotion_recognizer)\n",
    "    for param in emotion_recognizer.parameters():\n",
    "        param.requires_grad = True\n",
    "    emotion_recognizer.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "    lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.\n",
    "    best_uar = 0.\n",
    "    es = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Train and validate\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print('epoch: {}/{}'.format(epoch + 1, epochs))\n",
    "\n",
    "        train_loss, train_acc = train(train_loader, emotion_recognizer,\n",
    "                                        optimizer, criterion, device)\n",
    "        test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer,\n",
    "                                                criterion, device)\n",
    "\n",
    "        if verbose:\n",
    "            print('train_loss: {0:.5f}'.format(train_loss),\n",
    "                    'train_acc: {0:.3f}'.format(train_acc),\n",
    "                    'test_loss: {0:.5f}'.format(test_loss),\n",
    "                    'test_acc: {0:.3f}'.format(test_acc),\n",
    "                    'test_uar: {0:.3f}'.format(test_uar))\n",
    "\n",
    "        lr_schedule.step(test_loss)\n",
    "\n",
    "#         os.makedirs(os.path.join(opt.logger_path, opt.source_domain), exist_ok=True)\n",
    "\n",
    "#         model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'checkpoint.pth.tar')\n",
    "#         state = {'epoch': epoch+1, 'emotion_recognizer': emotion_recognizer.state_dict(), 'opt': opt}\n",
    "#         torch.save(state, model_file_name)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "#             model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'model.pth.tar')\n",
    "#             torch.save(state, model_file_name)\n",
    "\n",
    "            best_acc = test_acc\n",
    "\n",
    "        if test_uar > best_uar:\n",
    "            best_uar = test_uar\n",
    "\n",
    "        if es.step(test_loss):\n",
    "            break\n",
    "\n",
    "    return best_acc, best_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "consecutive-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just emotion task\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train(train_loader, model, optimizer, criterion, device, verbose = False):\n",
    "#     model.train()\n",
    "\n",
    "#     running_loss = 0.\n",
    "#     running_acc = 0.\n",
    "\n",
    "#     groundtruth = []\n",
    "#     prediction = []\n",
    "\n",
    "#     for i, train_data in enumerate(train_loader):\n",
    "#         visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = train_data # UPDATE\n",
    "\n",
    "#         visual_features = visual_features.to(device)\n",
    "#         acoustic_features = acoustic_features.to(device)\n",
    "#         lexical_features = lexical_features.to(device)\n",
    "\n",
    "#         labels = a_labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         emotion_output, stress_output = model(visual_features, acoustic_features, lexical_features)\n",
    "\n",
    "#         emotion_loss = criterion(emotion_output, labels)\n",
    "# #         stress_loss = criterion(stress_output, stress_labels)\n",
    "\n",
    "#         emotion_loss.backward()\n",
    "# #         stress_loss.backward()\n",
    "        \n",
    "#         optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "#         running_loss += emotion_loss.item()\n",
    "\n",
    "#         groundtruth.append(labels.tolist())\n",
    "#         predictions = emotion_output.argmax(dim=1, keepdim=True)\n",
    "#         prediction.append(predictions.view_as(labels).tolist())\n",
    "\n",
    "#         if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "#             print('.', flush=True, end='')\n",
    "            \n",
    "#     train_loss = running_loss / len(train_loader)\n",
    "\n",
    "#     groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "#     prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "#     train_acc = accuracy_score(prediction, groundtruth)\n",
    "\n",
    "#     return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "proper-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, criterion, device, verbose = False):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    emotion_running_loss = 0.\n",
    "    confound_running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    emotion_groundtruth = []\n",
    "    emotion_prediction = []\n",
    "    \n",
    "    confound_groundtruth = []\n",
    "    confound_prediction = []\n",
    "\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, d_labels, _ = train_data # UPDATE\n",
    "\n",
    "        visual_features = visual_features.to(device)\n",
    "        acoustic_features = acoustic_features.to(device)\n",
    "        lexical_features = lexical_features.to(device)\n",
    "\n",
    "        emotion_labels = a_labels.to(device)\n",
    "        confound_labels = d_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "\n",
    "        emotion_loss = criterion(emotion_predictions, emotion_labels)\n",
    "        confound_loss = criterion(confound_predictions, confound_labels)\n",
    "        loss = emotion_loss + confound_loss\n",
    "        \n",
    "        loss.backward()\n",
    "#         emotion_loss.backward()\n",
    "#         confound_loss.backward()\n",
    "        \n",
    "        optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "        emotion_running_loss += emotion_loss.item()\n",
    "        confound_running_loss += confound_loss.item()\n",
    "        running_loss += emotion_running_loss + confound_running_loss\n",
    "\n",
    "        emotion_groundtruth.append(emotion_labels.tolist())\n",
    "        emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "        emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\n",
    "        \n",
    "        confound_groundtruth.append(confound_labels.tolist())\n",
    "        confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\n",
    "        confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\n",
    "\n",
    "        if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "            print('.', flush=True, end='')\n",
    "        \n",
    "    emotion_loss = emotion_running_loss / len(train_loader)\n",
    "    confound_loss = confound_running_loss / len(train_loader)\n",
    "    loss = running_loss / len(train_loader)\n",
    "    train_loss = {'emotion_loss': emotion_loss,\n",
    "                  'confound_loss': confound_loss,\n",
    "                  'loss': loss\n",
    "                 }\n",
    "\n",
    "    emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\n",
    "    emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\n",
    "    \n",
    "    confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\n",
    "    confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\n",
    "\n",
    "    emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\n",
    "    confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\n",
    "    avg_acc = (emotion_acc + confound_acc) / 2\n",
    "    \n",
    "    train_acc = {'emotion_acc': emotion_acc,\n",
    "                  'confound_acc': confound_acc,\n",
    "                  'avg_acc': avg_acc\n",
    "                 }\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "wireless-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old test loss \n",
    "# def test(test_loader, model, criterion, device):\n",
    "#     model.eval()\n",
    "\n",
    "#     running_loss = 0.\n",
    "#     running_acc = 0.\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         groundtruth = []\n",
    "#         prediction = []\n",
    "\n",
    "#         for i, test_data in enumerate(test_loader):\n",
    "#             visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, _ = test_data # UPDATE\n",
    "\n",
    "#             visual_features = visual_features.to(device)\n",
    "#             acoustic_features = acoustic_features.to(device)\n",
    "#             lexical_features = lexical_features.to(device)\n",
    "\n",
    "#             labels = a_labels.to(device)\n",
    "\n",
    "#             emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "#             loss = criterion(emotion_predictions, labels)\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             groundtruth.append(labels.tolist())\n",
    "#             emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "#             prediction.append(emotion_predictions.view_as(labels).tolist())\n",
    "\n",
    "#         test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#         groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "#         prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "#         test_acc = accuracy_score(prediction, groundtruth)\n",
    "#         test_uar = recall_score(prediction, groundtruth, average='macro')\n",
    "\n",
    "#         return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "precise-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.\n",
    "    emotion_running_loss = 0.\n",
    "    confound_running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotion_groundtruth = []\n",
    "        emotion_prediction = []\n",
    "\n",
    "        confound_groundtruth = []\n",
    "        confound_prediction = []\n",
    "\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, _ = test_data # UPDATE\n",
    "\n",
    "            visual_features = visual_features.to(device)\n",
    "            acoustic_features = acoustic_features.to(device)\n",
    "            lexical_features = lexical_features.to(device)\n",
    "\n",
    "            emotion_labels = a_labels.to(device)\n",
    "            confound_labels = d_labels.to(device)\n",
    "\n",
    "            emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\n",
    "            \n",
    "            emotion_loss = criterion(emotion_predictions, emotion_labels)\n",
    "            confound_loss = criterion(confound_predictions, confound_labels)\n",
    "            loss = emotion_loss + confound_loss\n",
    "\n",
    "            emotion_running_loss += emotion_loss.item()\n",
    "            confound_running_loss += confound_loss.item()\n",
    "            running_loss += emotion_running_loss + confound_running_loss\n",
    "\n",
    "            emotion_groundtruth.append(emotion_labels.tolist())\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "            emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\n",
    "\n",
    "            confound_groundtruth.append(confound_labels.tolist())\n",
    "            confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\n",
    "            confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\n",
    "\n",
    "        emotion_loss = emotion_running_loss / len(train_loader)\n",
    "        confound_loss = confound_running_loss / len(train_loader)\n",
    "        loss = running_loss / len(train_loader)\n",
    "        test_loss = {'emotion_loss': emotion_loss,\n",
    "                     'confound_loss': confound_loss,\n",
    "                     'loss': loss\n",
    "                    }\n",
    "\n",
    "        emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\n",
    "        emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\n",
    "\n",
    "        confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\n",
    "        confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\n",
    "\n",
    "        emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\n",
    "        confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\n",
    "        avg_acc = (emotion_acc + confound_acc) / 2\n",
    "        test_acc = {'emotion_acc': emotion_acc,\n",
    "                    'confound_acc': confound_acc,\n",
    "                    'avg_acc': avg_acc\n",
    "                   }\n",
    "        \n",
    "        emotion_uar = recall_score(emotion_prediction, emotion_groundtruth, average='macro')\n",
    "        confound_uar = recall_score(confound_prediction, confound_groundtruth, average='macro')\n",
    "        avg_uar = (emotion_uar + confound_acc) / 2\n",
    "        test_uar = {'emotion_uar': emotion_uar,\n",
    "                    'confound_uar': confound_uar,\n",
    "                    'avg_uar': avg_uar\n",
    "                   }\n",
    "\n",
    "        return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "macro-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "driven-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emotion_acc': 0.3654501216545012, 'confound_acc': 0.2546228710462287, 'avg_acc': 0.31003649635036495}\n",
      "{'emotion_acc': 0.4597323600973236, 'confound_acc': 0.46618004866180046, 'avg_acc': 0.46295620437956203}\n",
      "{'emotion_acc': 0.5246958637469586, 'confound_acc': 0.4962287104622871, 'avg_acc': 0.5104622871046229}\n",
      "{'emotion_acc': 0.5497566909975669, 'confound_acc': 0.4937956204379562, 'avg_acc': 0.5217761557177616}\n",
      "{'emotion_acc': 0.5624087591240876, 'confound_acc': 0.5086374695863747, 'avg_acc': 0.5355231143552311}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-acb21e2d0c09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'emotion loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfound_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'confound loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# emotion_recognizer = net.Model(opt)\n",
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "emotion_loss = []\n",
    "confound_loss = []\n",
    "loss = []\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device)\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\n",
    "    confound_loss.append(train_loss['confound_loss'])\n",
    "    loss.append(train_loss['loss'])\n",
    "    print(train_acc)\n",
    "\n",
    "plt.plot(loss, label = 'train loss')\n",
    "plt.plot(emotion_loss, label = 'emotion loss')\n",
    "plt.plot(confound_loss, label = 'confound loss')\n",
    "\n",
    "\n",
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "reflected-fence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1138.2587376124432, 1110.687438991979, 1053.904150089459, 1031.2967825915796, 1023.8325972871905] [1.0947116059088058, 1.0668253218501458, 1.0039617843665039, 0.9766656892077005, 0.9669124998935001] [1.1143799787133584, 1.0749375056547876, 1.0354727056703679, 1.024888481379483, 1.0171976953976813]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss, label = 'train loss')\n",
    "plt.plot(emotion_loss, label = 'emotion loss')\n",
    "plt.plot(confound_loss, label = 'confound loss')\n",
    "print(loss,emotion_loss,confound_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "authorized-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "uar = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accurate-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-47f3845f0cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasterNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macoustic_modality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexical_modality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_modality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_uar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_uar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4d2d13efd8ec>\u001b[0m in \u001b[0;36mtrain_one_folder\u001b[0;34m(model, folder, epochs, verbose, learning_rate, patience)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: {}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         train_loss, train_acc = train(train_loader, emotion_recognizer,\n\u001b[0m\u001b[1;32m     33\u001b[0m                                         optimizer, criterion, device)\n\u001b[1;32m     34\u001b[0m         test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer,\n",
      "\u001b[0;32m<ipython-input-12-974f2f3f2ca2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, device, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#         stress_loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# do we need two optimizers?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0memotion_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/stressed_emotion/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/stressed_emotion/.venv/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/stressed_emotion/.venv/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 0, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 1, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 2, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 3, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 4, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(5),acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(5),uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([acc,uar]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-continuity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
