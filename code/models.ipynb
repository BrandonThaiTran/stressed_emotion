{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confidential-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "\n",
    "import sampler\n",
    "import datasets\n",
    "from earlystopping import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from torch.autograd  import  Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-pantyhose",
   "metadata": {},
   "source": [
    "# Acoustic Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-millennium",
   "metadata": {},
   "source": [
    "Inputs for acoustic branch will be N x 40 where N [1,33]  \n",
    "Time step: (2, 10) (seconds?)  \n",
    "N: relative duration after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designed-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticNet(nn.Module):\n",
    "    def __init__(self, num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2):\n",
    "        super(AcousticNet, self).__init__()\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.convs = [self.conv1, self.conv2, self.conv3, self.conv4]\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=conv_width,hidden_size=32,num_layers=num_gru_layers) # 19 is hardcoded\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "#         print(x.shape)\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.relu(self.max_pool(self.convs[i](x)))\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = F.adaptive_avg_pool1d(x,1)[:, :, -1]\n",
    "#         x = self.mean_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abandoned-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = AcousticNet(num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "test_vec = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "output = net(test_vec)\n",
    "print(f'Shape of output: {output.shape}')\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-census",
   "metadata": {},
   "source": [
    "# Lexical Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "african-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement GRU (or transformer)\n",
    "class LexicalNet(nn.Module):\n",
    "    def __init__(self, num_gru_layers = 2):\n",
    "        super(LexicalNet, self).__init__()\n",
    "        # implement GRU (or transformer)\n",
    "        self.gru = nn.GRU(input_size=768,hidden_size=32,num_layers=num_gru_layers)\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2) \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "#         x = self.mean_pool(x)\n",
    "        x = self.flatten(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wrapped-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dummy input\n",
    "net = LexicalNet(num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "test_vec = torch.randn(batch_size, 1, 768)\n",
    "output = net(test_vec)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-webmaster",
   "metadata": {},
   "source": [
    "# Master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "integral-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRL(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self,x):\n",
    "#         return x\n",
    "#     @staticmethod\n",
    "#     def backward(self,grad_output):\n",
    "#         grad_input = grad_output.neg()\n",
    "#         return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chronic-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "    \n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "senior-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterNet(nn.Module):\n",
    "    def __init__(self, acoustic_modality = True, lexical_modality = True, visual_modality = False,\n",
    "                 num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2,\n",
    "                 num_dense_layers = 1, dense_layer_width = 32, grl_lambda = .3):\n",
    "        super(MasterNet, self).__init__()\n",
    "        \n",
    "        self.acoustic_modality = acoustic_modality\n",
    "        self.lexical_modality = lexical_modality\n",
    "        self.visual_modality = visual_modality\n",
    "        \n",
    "        self.acoustic_model = AcousticNet(num_conv_layers = num_conv_layers, kernel_size = kernel_size, \n",
    "                                     conv_width = conv_width, num_gru_layers = num_gru_layers)\n",
    "        self.lexical_model = LexicalNet(num_gru_layers = 2)\n",
    "        \n",
    "        # emotion classifier\n",
    "#         self.dense1_emo = nn.Linear()\n",
    "#         self.dense2_emo = nn.Linear()\n",
    "        \n",
    "        width = 0 # width of the FC layers\n",
    "        if self.acoustic_modality:\n",
    "            width += 32\n",
    "        if self.visual_modality:\n",
    "            width += 0 # to implement\n",
    "        if self.lexical_modality:\n",
    "            width += 32\n",
    "            \n",
    "        self.fc_1 = nn.Linear(width, dense_layer_width)\n",
    "        self.fc_2 = nn.Linear(dense_layer_width, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "#         # To implement   \n",
    "#         if num_dense_layers == 2:\n",
    "#             self.fc = nn.Sequential()\n",
    "#             self.linear_1 = nn.Linear(width, dense_layer_width)\n",
    "#         else:\n",
    "#             self.fc = \n",
    "        \n",
    "        # confound classifier -- to implement\n",
    "        \n",
    "        self.grl = GradientReversal(lambda_ = grl_lambda)\n",
    "        self.dense_con = nn.Linear(width, 3)\n",
    "#         self.dense2_con = None\n",
    "        \n",
    "        \n",
    "    def forward_a(self, x_a):\n",
    "        x = x_a\n",
    "        x = self.acoustic_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_l(self, x_l):\n",
    "        x = torch.unsqueeze(x_l, dim = 1)\n",
    "        x = self.lexical_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_v(self, x_v):\n",
    "        x = x_v\n",
    "        return x\n",
    "    \n",
    "    def encoder(self, x_v, x_a, x_l):\n",
    "#         print('x_a before encoding', x_a.shape)\n",
    "#         print('x_l before encoding', x_l.shape)\n",
    "        if self.visual_modality:\n",
    "            x_v = self.forward_v(x_v)\n",
    "        if self.acoustic_modality:\n",
    "            x_a = self.forward_a(x_a)\n",
    "        if self.lexical_modality:\n",
    "            x_l = self.forward_l(x_l)\n",
    "#         print('x_a after encoding', x_a.shape)\n",
    "#         print('x_l after encoding', x_l.shape)\n",
    "        \n",
    "        if self.visual_modality:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = torch.cat((x_v, x_a), 1)\n",
    "            else:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_l), 1)\n",
    "                else:\n",
    "                    x = x_v\n",
    "        else:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = x_a\n",
    "            else:\n",
    "                x = x_l\n",
    "#         print('x after concat', x.shape)\n",
    "        return x\n",
    "\n",
    "    def stress_model(self, x):\n",
    "#         x = self.grl.apply(x)\n",
    "        x = self.grl(x)\n",
    "        x = self.dense_con(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def recognizer(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_v, x_a, x_l):\n",
    "        x = self.encoder(x_v, x_a, x_l)\n",
    "        emotion_output = self.recognizer(x)\n",
    "        stress_output = self.stress_model(x)\n",
    "        \n",
    "        return emotion_output, stress_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "parallel-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of emotion output: torch.Size([8, 3])\n",
      "Shape of stress output: torch.Size([8, 3])\n",
      "tensor([[0.3534, 0.3640, 0.2826],\n",
      "        [0.3575, 0.3649, 0.2776],\n",
      "        [0.3651, 0.3563, 0.2786],\n",
      "        [0.3572, 0.3555, 0.2873],\n",
      "        [0.3597, 0.3548, 0.2856],\n",
      "        [0.3552, 0.3631, 0.2817],\n",
      "        [0.3590, 0.3665, 0.2745],\n",
      "        [0.3639, 0.3619, 0.2741]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3370, 0.3587, 0.3043],\n",
      "        [0.3259, 0.3731, 0.3011],\n",
      "        [0.3104, 0.3787, 0.3109],\n",
      "        [0.3268, 0.3415, 0.3317],\n",
      "        [0.3518, 0.3380, 0.3102],\n",
      "        [0.3893, 0.3125, 0.2982],\n",
      "        [0.3619, 0.3347, 0.3034],\n",
      "        [0.3488, 0.3033, 0.3479]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = MasterNet()\n",
    "batch_size = 8\n",
    "n_acoustic_channels = 40\n",
    "duration_acoustic = 1232\n",
    "acoustic_features = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\n",
    "# lexical_features = torch.randn(batch_size, 1, 300)\n",
    "lexical_features = torch.randn(batch_size, 768)\n",
    "visual_features = None\n",
    "emotion_output, stress_output = net(visual_features, acoustic_features, lexical_features)\n",
    "print(f'Shape of emotion output: {emotion_output.shape}')\n",
    "print(f'Shape of stress output: {stress_output.shape}')\n",
    "print(emotion_output)\n",
    "print(stress_output)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "appreciated-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use specific GPU\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():  \n",
    "        dev = \"cuda:0\" \n",
    "    else:  \n",
    "        dev = \"cpu\"  \n",
    "    return torch.device(dev)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prescription-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_folder(model, folder = 0, epochs = 1, verbose = False, learning_rate = 1e-4, patience = 5):\n",
    "    # Use specific GPU\n",
    "    device = get_device()\n",
    "\n",
    "    # Dataloaders    \n",
    "    train_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/train.csv'\n",
    "    train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "    test_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/test.csv'\n",
    "    test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "    # Model, optimizer and loss function\n",
    "    emotion_recognizer = model\n",
    "    init_weights(emotion_recognizer)\n",
    "    for param in emotion_recognizer.parameters():\n",
    "        param.requires_grad = True\n",
    "    emotion_recognizer.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "    lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.\n",
    "    best_uar = 0.\n",
    "    es = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Train and validate\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print('epoch: {}/{}'.format(epoch + 1, epochs))\n",
    "\n",
    "        train_loss, train_acc = train(train_loader, emotion_recognizer,\n",
    "                                        optimizer, criterion, device)\n",
    "        test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer,\n",
    "                                                criterion, device)\n",
    "\n",
    "        if verbose:\n",
    "            print('train_loss: {0:.5f}'.format(train_loss),\n",
    "                    'train_acc: {0:.3f}'.format(train_acc),\n",
    "                    'test_loss: {0:.5f}'.format(test_loss),\n",
    "                    'test_acc: {0:.3f}'.format(test_acc),\n",
    "                    'test_uar: {0:.3f}'.format(test_uar))\n",
    "\n",
    "        lr_schedule.step(test_loss)\n",
    "\n",
    "#         os.makedirs(os.path.join(opt.logger_path, opt.source_domain), exist_ok=True)\n",
    "\n",
    "#         model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'checkpoint.pth.tar')\n",
    "#         state = {'epoch': epoch+1, 'emotion_recognizer': emotion_recognizer.state_dict(), 'opt': opt}\n",
    "#         torch.save(state, model_file_name)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "#             model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'model.pth.tar')\n",
    "#             torch.save(state, model_file_name)\n",
    "\n",
    "            best_acc = test_acc\n",
    "\n",
    "        if test_uar > best_uar:\n",
    "            best_uar = test_uar\n",
    "\n",
    "        if es.step(test_loss):\n",
    "            break\n",
    "\n",
    "    return best_acc, best_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lesbian-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, criterion, device, verbose = False):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    groundtruth = []\n",
    "    prediction = []\n",
    "\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = train_data # UPDATE\n",
    "\n",
    "        visual_features = visual_features.to(device)\n",
    "        acoustic_features = acoustic_features.to(device)\n",
    "        lexical_features = lexical_features.to(device)\n",
    "\n",
    "        labels = a_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emotion_output, stress_output = model(visual_features, acoustic_features, lexical_features)\n",
    "\n",
    "        emotion_loss = criterion(emotion_output, labels)\n",
    "#         stress_loss = criterion(stress_output, stress_labels)\n",
    "\n",
    "        emotion_loss.backward()\n",
    "#         stress_loss.backward()\n",
    "        \n",
    "        optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "        running_loss += emotion_loss.item()\n",
    "\n",
    "        groundtruth.append(labels.tolist())\n",
    "        predictions = emotion_output.argmax(dim=1, keepdim=True)\n",
    "        prediction.append(predictions.view_as(labels).tolist())\n",
    "\n",
    "        if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "            print('.', flush=True, end='')\n",
    "            \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "    prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "    train_acc = accuracy_score(prediction, groundtruth)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "municipal-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        groundtruth = []\n",
    "        prediction = []\n",
    "\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = test_data # UPDATE\n",
    "\n",
    "            visual_features = visual_features.to(device)\n",
    "            acoustic_features = acoustic_features.to(device)\n",
    "            lexical_features = lexical_features.to(device)\n",
    "\n",
    "            labels = a_labels.to(device)\n",
    "\n",
    "            emotion_predictions, _ = model(visual_features, acoustic_features, lexical_features)\n",
    "            loss = criterion(emotion_predictions, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            groundtruth.append(labels.tolist())\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "            prediction.append(emotion_predictions.view_as(labels).tolist())\n",
    "\n",
    "        test_loss = running_loss / len(test_loader)\n",
    "\n",
    "        groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "        prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "        test_acc = accuracy_score(prediction, groundtruth)\n",
    "        test_uar = recall_score(prediction, groundtruth, average='macro')\n",
    "\n",
    "        return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "official-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "auburn-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.383698296836983\n",
      "0.4925783397471138\n"
     ]
    }
   ],
   "source": [
    "# emotion_recognizer = net.Model(opt)\n",
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device)\n",
    "    print(train_acc)\n",
    "\n",
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "complete-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.383698296836983 0.4925783397471138\n"
     ]
    }
   ],
   "source": [
    "print(train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "minus-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "uar = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "lucky-collector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n",
      "train_loss: 1.09713 train_acc: 0.350 test_loss: 1.09258 test_acc: 0.385 test_uar: 0.497\n",
      "epoch: 2/10\n",
      "train_loss: 1.05485 train_acc: 0.480 test_loss: 0.99448 test_acc: 0.608 test_uar: 0.526\n",
      "epoch: 3/10\n",
      "train_loss: 1.00438 train_acc: 0.522 test_loss: 0.95767 test_acc: 0.616 test_uar: 0.583\n",
      "epoch: 4/10\n",
      "train_loss: 0.98472 train_acc: 0.543 test_loss: 0.95484 test_acc: 0.601 test_uar: 0.547\n",
      "epoch: 5/10\n",
      "train_loss: 0.97051 train_acc: 0.561 test_loss: 0.94535 test_acc: 0.607 test_uar: 0.555\n",
      "epoch: 6/10\n",
      "train_loss: 0.95275 train_acc: 0.583 test_loss: 0.94903 test_acc: 0.592 test_uar: 0.564\n",
      "epoch: 7/10\n",
      "train_loss: 0.94403 train_acc: 0.597 test_loss: 0.95594 test_acc: 0.574 test_uar: 0.567\n",
      "epoch: 8/10\n",
      "train_loss: 0.93429 train_acc: 0.607 test_loss: 0.95236 test_acc: 0.582 test_uar: 0.568\n",
      "epoch: 9/10\n",
      "train_loss: 0.93019 train_acc: 0.616 test_loss: 0.94957 test_acc: 0.579 test_uar: 0.557\n",
      "epoch: 10/10\n",
      "train_loss: 0.93084 train_acc: 0.613 test_loss: 0.94949 test_acc: 0.580 test_uar: 0.557\n"
     ]
    }
   ],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 0, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "premier-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n",
      "train_loss: 1.09677 train_acc: 0.360 test_loss: 1.07883 test_acc: 0.486 test_uar: 0.456\n",
      "epoch: 2/10\n",
      "train_loss: 1.05017 train_acc: 0.475 test_loss: 0.98161 test_acc: 0.574 test_uar: 0.516\n",
      "epoch: 3/10\n",
      "train_loss: 1.00571 train_acc: 0.515 test_loss: 0.96012 test_acc: 0.592 test_uar: 0.521\n",
      "epoch: 4/10\n",
      "train_loss: 0.98320 train_acc: 0.544 test_loss: 0.94661 test_acc: 0.584 test_uar: 0.499\n",
      "epoch: 5/10\n",
      "train_loss: 0.96794 train_acc: 0.565 test_loss: 0.94390 test_acc: 0.588 test_uar: 0.536\n",
      "epoch: 6/10\n",
      "train_loss: 0.95261 train_acc: 0.585 test_loss: 0.94285 test_acc: 0.580 test_uar: 0.540\n",
      "epoch: 7/10\n",
      "train_loss: 0.94285 train_acc: 0.596 test_loss: 0.95042 test_acc: 0.565 test_uar: 0.539\n",
      "epoch: 8/10\n",
      "train_loss: 0.93306 train_acc: 0.610 test_loss: 0.94771 test_acc: 0.578 test_uar: 0.553\n",
      "epoch: 9/10\n",
      "train_loss: 0.91846 train_acc: 0.629 test_loss: 0.95011 test_acc: 0.571 test_uar: 0.552\n",
      "epoch: 10/10\n",
      "train_loss: 0.93281 train_acc: 0.607 test_loss: 0.95069 test_acc: 0.564 test_uar: 0.542\n"
     ]
    }
   ],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 1, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "expanded-scope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n",
      "train_loss: 1.09326 train_acc: 0.378 test_loss: 1.06696 test_acc: 0.494 test_uar: 0.463\n",
      "epoch: 2/10\n",
      "train_loss: 1.03375 train_acc: 0.503 test_loss: 0.99560 test_acc: 0.551 test_uar: 0.579\n",
      "epoch: 3/10\n",
      "train_loss: 1.00079 train_acc: 0.524 test_loss: 0.97370 test_acc: 0.562 test_uar: 0.498\n",
      "epoch: 4/10\n",
      "train_loss: 0.98162 train_acc: 0.547 test_loss: 0.96379 test_acc: 0.564 test_uar: 0.519\n",
      "epoch: 5/10\n",
      "train_loss: 0.97818 train_acc: 0.556 test_loss: 0.96088 test_acc: 0.567 test_uar: 0.535\n",
      "epoch: 6/10\n",
      "train_loss: 0.95843 train_acc: 0.571 test_loss: 0.96754 test_acc: 0.551 test_uar: 0.543\n",
      "epoch: 7/10\n",
      "train_loss: 0.93942 train_acc: 0.595 test_loss: 0.95876 test_acc: 0.556 test_uar: 0.541\n",
      "epoch: 8/10\n",
      "train_loss: 0.93177 train_acc: 0.610 test_loss: 0.95935 test_acc: 0.558 test_uar: 0.544\n",
      "epoch: 9/10\n",
      "train_loss: 0.92904 train_acc: 0.615 test_loss: 0.96391 test_acc: 0.548 test_uar: 0.546\n",
      "epoch: 10/10\n",
      "train_loss: 0.92017 train_acc: 0.624 test_loss: 0.96648 test_acc: 0.548 test_uar: 0.546\n"
     ]
    }
   ],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 2, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "herbal-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n",
      "train_loss: 1.09577 train_acc: 0.349 test_loss: 1.06419 test_acc: 0.491 test_uar: 0.423\n",
      "epoch: 2/10\n",
      "train_loss: 1.03375 train_acc: 0.494 test_loss: 1.00374 test_acc: 0.530 test_uar: 0.468\n",
      "epoch: 3/10\n",
      "train_loss: 0.99176 train_acc: 0.529 test_loss: 0.98995 test_acc: 0.541 test_uar: 0.454\n",
      "epoch: 4/10\n",
      "train_loss: 0.97208 train_acc: 0.556 test_loss: 0.98615 test_acc: 0.529 test_uar: 0.495\n",
      "epoch: 5/10\n",
      "train_loss: 0.95899 train_acc: 0.574 test_loss: 0.98135 test_acc: 0.538 test_uar: 0.496\n",
      "epoch: 6/10\n",
      "train_loss: 0.94146 train_acc: 0.600 test_loss: 0.99087 test_acc: 0.518 test_uar: 0.494\n",
      "epoch: 7/10\n",
      "train_loss: 0.94314 train_acc: 0.593 test_loss: 0.97982 test_acc: 0.529 test_uar: 0.503\n",
      "epoch: 8/10\n",
      "train_loss: 0.92680 train_acc: 0.618 test_loss: 0.98756 test_acc: 0.522 test_uar: 0.487\n",
      "epoch: 9/10\n",
      "train_loss: 0.91297 train_acc: 0.633 test_loss: 0.98669 test_acc: 0.523 test_uar: 0.499\n",
      "epoch: 10/10\n",
      "train_loss: 0.90907 train_acc: 0.636 test_loss: 0.98580 test_acc: 0.528 test_uar: 0.505\n"
     ]
    }
   ],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 3, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "requested-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n",
      "train_loss: 1.09494 train_acc: 0.364 test_loss: 1.08246 test_acc: 0.482 test_uar: 0.486\n",
      "epoch: 2/10\n",
      "train_loss: 1.03453 train_acc: 0.500 test_loss: 1.00392 test_acc: 0.540 test_uar: 0.498\n",
      "epoch: 3/10\n",
      "train_loss: 0.99740 train_acc: 0.531 test_loss: 0.98673 test_acc: 0.541 test_uar: 0.470\n",
      "epoch: 4/10\n",
      "train_loss: 0.97844 train_acc: 0.548 test_loss: 0.98851 test_acc: 0.528 test_uar: 0.485\n",
      "epoch: 5/10\n",
      "train_loss: 0.96337 train_acc: 0.576 test_loss: 0.97785 test_acc: 0.534 test_uar: 0.494\n",
      "epoch: 6/10\n",
      "train_loss: 0.95083 train_acc: 0.585 test_loss: 0.98297 test_acc: 0.535 test_uar: 0.505\n",
      "epoch: 7/10\n",
      "train_loss: 0.94385 train_acc: 0.593 test_loss: 0.97981 test_acc: 0.530 test_uar: 0.505\n",
      "epoch: 8/10\n",
      "train_loss: 0.93303 train_acc: 0.609 test_loss: 0.97709 test_acc: 0.536 test_uar: 0.510\n",
      "epoch: 9/10\n",
      "train_loss: 0.92573 train_acc: 0.620 test_loss: 0.98014 test_acc: 0.526 test_uar: 0.504\n",
      "epoch: 10/10\n",
      "train_loss: 0.93425 train_acc: 0.607 test_loss: 0.97914 test_acc: 0.528 test_uar: 0.505\n"
     ]
    }
   ],
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 4, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "textile-manchester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(5),acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "composed-selling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(5),uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "functional-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6157229246838922, 0.5919381557150746, 0.5669475655430711, 0.5406562054208274, 0.5410138248847927]\n",
      "[0.5826526522644119, 0.5532637338288996, 0.5787538872066461, 0.5047373306364468, 0.5097780299334587]\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "altered-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "indoor-proceeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615723</td>\n",
       "      <td>0.582653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.591938</td>\n",
       "      <td>0.553264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566948</td>\n",
       "      <td>0.578754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.540656</td>\n",
       "      <td>0.504737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.541014</td>\n",
       "      <td>0.509778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.615723  0.582653\n",
       "1  0.591938  0.553264\n",
       "2  0.566948  0.578754\n",
       "3  0.540656  0.504737\n",
       "4  0.541014  0.509778"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([acc,uar]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fluid-casino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5712557352495317"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "marked-division",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5458371267739726"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-clark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
