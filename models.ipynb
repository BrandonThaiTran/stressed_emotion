{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "theoretical-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sampler\n",
    "import datasets\n",
    "\n",
    "from torch.autograd  import  Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-vienna",
   "metadata": {},
   "source": [
    "# Acoustic Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-preview",
   "metadata": {},
   "source": [
    "Inputs for acoustic branch will be N x 40 where N [1,33]  \n",
    "Time step: (2, 10) (seconds?)  \n",
    "N: relative duration after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electronic-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticNet(nn.Module):\n",
    "    def __init__(self, num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2):\n",
    "        super(AcousticNet, self).__init__()\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.convs = [self.conv1, self.conv2, self.conv3, self.conv4]\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=conv_width,hidden_size=32,num_layers=num_gru_layers) # 19 is hardcoded\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.relu(self.max_pool(self.convs[i](x)))\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = F.adaptive_avg_pool1d(x,1)[:, :, -1]\n",
    "#         x = self.mean_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compact-investment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = AcousticNet(num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "test_vec = torch.randn(batch_size, 40, 17) # samples x features (or channels) x N (relative duration)\n",
    "output = net(test_vec)\n",
    "print(f'Shape of output: {output.shape}')\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-recorder",
   "metadata": {},
   "source": [
    "# Lexical Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adopted-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement GRU (or transformer)\n",
    "class LexicalNet(nn.Module):\n",
    "    def __init__(self, num_gru_layers = 2):\n",
    "        super(LexicalNet, self).__init__()\n",
    "        # implement GRU (or transformer)\n",
    "        self.gru = nn.GRU(input_size=300,hidden_size=32,num_layers=num_gru_layers)\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2) \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "#         x = self.mean_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pointed-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = LexicalNet(num_gru_layers = 2)\n",
    "batch_size = 8\n",
    "test_vec = torch.randn(batch_size, 1, 300)\n",
    "output = net(test_vec)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-enzyme",
   "metadata": {},
   "source": [
    "# Master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accessory-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL(Function):\n",
    "    @staticmethod\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def backward(self,grad_output):\n",
    "        grad_input = grad_output.neg()\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "    \n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "nuclear-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterNet(nn.Module):\n",
    "    def __init__(self, acoustic_modality = True, lexical_modality = True, visual_modality = False,\n",
    "                 num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2,\n",
    "                 num_dense_layers = 1, dense_layer_width = 32, grl_lambda = .3):\n",
    "        super(MasterNet, self).__init__()\n",
    "        \n",
    "        self.acoustic_modality = acoustic_modality\n",
    "        self.lexical_modality = lexical_modality\n",
    "        self.visual_modality = visual_modality\n",
    "        \n",
    "        self.acoustic_model = AcousticNet(num_conv_layers = num_conv_layers, kernel_size = kernel_size, \n",
    "                                     conv_width = conv_width, num_gru_layers = num_gru_layers)\n",
    "        self.lexical_model = LexicalNet(num_gru_layers = 2)\n",
    "        \n",
    "        # emotion classifier\n",
    "#         self.dense1_emo = nn.Linear()\n",
    "#         self.dense2_emo = nn.Linear()\n",
    "        \n",
    "        width = 0 # width of the FC layers\n",
    "        if self.acoustic_modality:\n",
    "            width += 32\n",
    "        if self.visual_modality:\n",
    "            width += 0 # to implement\n",
    "        if self.lexical_modality:\n",
    "            width += 32\n",
    "            \n",
    "        self.fc_1 = nn.Linear(width, dense_layer_width)\n",
    "        self.fc_2 = nn.Linear(dense_layer_width, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "#         # To implement   \n",
    "#         if num_dense_layers == 2:\n",
    "#             self.fc = nn.Sequential()\n",
    "#             self.linear_1 = nn.Linear(width, dense_layer_width)\n",
    "#         else:\n",
    "#             self.fc = \n",
    "        \n",
    "        # confound classifier -- to implement\n",
    "        \n",
    "        self.grl = GRL()\n",
    "        self.dense_con = nn.Linear(width, 3)\n",
    "#         self.dense2_con = None\n",
    "        \n",
    "        \n",
    "    def forward_a(self, x_a):\n",
    "        x = x_a\n",
    "        x = self.acoustic_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_l(self, x_l):\n",
    "        x = x_l\n",
    "        x = self.lexical_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_v(self, x_v):\n",
    "        x = x_v\n",
    "        return x\n",
    "    \n",
    "    def encoder(self, x_v, x_a, x_l):\n",
    "        print('x_a before encoding', x_a.shape)\n",
    "        print('x_l before encoding', x_l.shape)\n",
    "        if self.visual_modality:\n",
    "            x_v = self.forward_v(x_v)\n",
    "        if self.acoustic_modality:\n",
    "            x_a = self.forward_a(x_a)\n",
    "        if self.lexical_modality:\n",
    "            x_l = self.forward_l(x_l)\n",
    "        print('x_a after encoding', x_a.shape)\n",
    "        print('x_l after encoding', x_l.shape)\n",
    "        \n",
    "        if self.visual_modality:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = torch.cat((x_v, x_a), 1)\n",
    "            else:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_l), 1)\n",
    "                else:\n",
    "                    x = x_v\n",
    "        else:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = x_a\n",
    "            else:\n",
    "                x = x_l\n",
    "        print('x after concat', x.shape)\n",
    "        return x\n",
    "\n",
    "    def stress_model(self, x):\n",
    "        x = self.grl.apply(x)\n",
    "        x = self.dense_con(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def recognizer(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_v, x_a, x_l):\n",
    "        x = self.encoder(x_v, x_a, x_l)\n",
    "        emotion_output = self.recognizer(x)\n",
    "        stress_output = self.stress_model(x)\n",
    "        \n",
    "        return emotion_output, stress_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "random-works",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_a before encoding torch.Size([8, 40, 17])\n",
      "x_l before encoding torch.Size([8, 1, 300])\n",
      "torch.Size([8, 32])\n",
      "x_a after encoding torch.Size([8, 32])\n",
      "x_l after encoding torch.Size([8, 32])\n",
      "x after concat torch.Size([8, 64])\n",
      "torch.Size([8, 64])\n",
      "Shape of emotion output: torch.Size([8, 3])\n",
      "Shape of stress output: torch.Size([8, 3])\n",
      "tensor([[0.3092, 0.3467, 0.3441],\n",
      "        [0.3127, 0.3483, 0.3390],\n",
      "        [0.3138, 0.3497, 0.3365],\n",
      "        [0.3063, 0.3614, 0.3323],\n",
      "        [0.3061, 0.3597, 0.3342],\n",
      "        [0.3089, 0.3606, 0.3305],\n",
      "        [0.3080, 0.3599, 0.3321],\n",
      "        [0.3037, 0.3634, 0.3329]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3399, 0.3367, 0.3233],\n",
      "        [0.3549, 0.3074, 0.3377],\n",
      "        [0.3672, 0.3226, 0.3102],\n",
      "        [0.3508, 0.3366, 0.3127],\n",
      "        [0.3364, 0.3411, 0.3225],\n",
      "        [0.2944, 0.3969, 0.3087],\n",
      "        [0.2811, 0.4005, 0.3184],\n",
      "        [0.2838, 0.3868, 0.3294]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = MasterNet()\n",
    "batch_size = 8\n",
    "acoustic_features = torch.randn(batch_size, 40, 17) # samples x features (or channels) x N (relative duration)\n",
    "lexical_features = torch.randn(batch_size, 1, 300)\n",
    "visual_features = None\n",
    "emotion_output, stress_output = net(visual_features, acoustic_features, lexical_features)\n",
    "print(f'Shape of emotion output: {emotion_output.shape}')\n",
    "print(f'Shape of stress output: {stress_output.shape}')\n",
    "print(emotion_output)\n",
    "print(stress_output)\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, criterion, device, opt):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    groundtruth = []\n",
    "    prediction = []\n",
    "\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = train_data # UPDATE\n",
    "\n",
    "        visual_features = visual_features.to(device)\n",
    "        acoustic_features = acoustic_features.to(device)\n",
    "        lexical_features = lexical_features.to(device)\n",
    "\n",
    "        labels = a_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emotion_output, stress_output = model(visual_features, acoustic_features, lexical_features)\n",
    "\n",
    "        emotion_loss = criterion(emotion_output, emotion_labels)\n",
    "        stress_loss = criterion(stress_output, stress_labels)\n",
    "\n",
    "        emotion_loss.backward()\n",
    "        stress_loss.backward()\n",
    "        \n",
    "        optimizer.step() # do we need two optimizers?\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        groundtruth.append(labels.tolist())\n",
    "        predictions = predictions.argmax(dim=1, keepdim=True)\n",
    "        prediction.append(predictions.view_as(labels).tolist())\n",
    "\n",
    "        if opt.verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\n",
    "            print('.', flush=True, end='')\n",
    "            \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "    prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "    train_acc = accuracy_score(prediction, groundtruth)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device, opt):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        groundtruth = []\n",
    "        prediction = []\n",
    "\n",
    "    for i, test_data in enumerate(test_loader):\n",
    "            visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = test_data # UPDATE\n",
    "\n",
    "            visual_features = visual_features.to(device)\n",
    "            acoustic_features = acoustic_features.to(device)\n",
    "            lexical_features = lexical_features.to(device)\n",
    "\n",
    "            labels = a_labels.to(device)\n",
    "\n",
    "            emotion_predictions, _ = model(visual_features, acoustic_features, lexical_features)\n",
    "            loss = criterion(emotion_predictions, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            groundtruth.append(labels.tolist())\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\n",
    "            prediction.append(emotion_predictions.view_as(labels).tolist())\n",
    "\n",
    "        test_loss = running_loss / len(test_loader)\n",
    "\n",
    "        groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "        prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "        test_acc = accuracy_score(prediction, groundtruth)\n",
    "        test_uar = recall_score(prediction, groundtruth, average='macro')\n",
    "        \n",
    "        return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-asbestos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solid-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "permanent-disability",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-150fd7c60232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memotion_recognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_recognizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0memotion_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# emotion_recognizer = net.Model(opt)\n",
    "emotion_recognizer = MasterNet()\n",
    "init_weights(emotion_recognizer)\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=opt.learning_rate)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset_file_path = '../yufeng_emotions/dataset/IEMOCAP/0/train.csv'\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\n",
    "test_dataset_file_path = '../yufeng_emotions/dataset/IEMOCAP/0/test.csv'\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\n",
    "\n",
    "train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device, opt)\n",
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-saint",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
