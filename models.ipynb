{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-chaos",
   "metadata": {},
   "source": [
    "# Master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "opened-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterNet(nn.Module):\n",
    "    def __init__(self, acoustic_modality = True, lexical_modality = True, visual_modality = False,\n",
    "                 num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2,\n",
    "                 num_dense_layers = 1, dense_layer_width = 32, grl_lambda = .3):\n",
    "        super(MasterNet, self).__init__()\n",
    "        \n",
    "        self.acoustic_modality = acoustic_modality\n",
    "        self.lexical_modality = lexical_modality\n",
    "        self.visual_modality = visual_modality\n",
    "        \n",
    "        self.acoustic_model = AcousticNet(num_conv_layers = num_conv_layers, kernel_size = kernel_size, \n",
    "                                     conv_width = conv_width, num_gru_layers = num_gru_layers)\n",
    "        self.lexical_model = LexicalNet(num_gru_layers = 2)\n",
    "        \n",
    "        # emotion classifier\n",
    "#         self.dense1_emo = nn.Linear()\n",
    "#         self.dense2_emo = nn.Linear()\n",
    "        \n",
    "        # confound classifier -- to implement\n",
    "        self.grl = None\n",
    "        self.dense1_con = None\n",
    "        self.dense2_con = None\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward_a(self, x_a):\n",
    "        x = x_a\n",
    "        x = self.acoustic_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_l(self, x_l):\n",
    "        x = x_l\n",
    "        x = self.lexical_model(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_v(self, x_v):\n",
    "        x = x_v\n",
    "        return x\n",
    "    \n",
    "    def encoder(self, x_v, x_a, x_l):\n",
    "        print('x_a before encoding', x_a.shape)\n",
    "        print('x_l before encoding', x_l.shape)\n",
    "        if self.visual_modality:\n",
    "            x_v = self.forward_v(x_v)\n",
    "        if self.acoustic_modality:\n",
    "            x_a = self.forward_a(x_a)\n",
    "        if self.lexical_modality:\n",
    "            x_l = self.forward_l(x_l)\n",
    "        print('x_a after encoding', x_a.shape)\n",
    "        print('x_l after encoding', x_l.shape)\n",
    "        \n",
    "        if self.visual_modality:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = torch.cat((x_v, x_a), 1)\n",
    "            else:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_v, x_l), 1)\n",
    "                else:\n",
    "                    x = x_v\n",
    "        else:\n",
    "            if self.acoustic_modality:\n",
    "                if self.lexical_modality:\n",
    "                    x = torch.cat((x_a, x_l), 1)\n",
    "                else:\n",
    "                    x = x_a\n",
    "            else:\n",
    "                x = x_l\n",
    "        print('x after concat', x.shape)\n",
    "        return x\n",
    "\n",
    "    def recognizer(self, x):\n",
    "#         x = self.relu(self.linear_1(x))\n",
    "#         x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_v, x_a, x_l):\n",
    "        x = self.encoder(x_v, x_a, x_l)\n",
    "        print('after encoding', x.shape)\n",
    "        x = self.recognizer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "central-edwards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_a before encoding torch.Size([10, 40, 17])\n",
      "x_l before encoding torch.Size([10, 1, 300])\n",
      "initial x_a shape torch.Size([10, 40, 17])\n",
      "after final conv torch.Size([10, 32, 3])\n",
      "after transpose torch.Size([10, 3, 32])\n",
      "gru torch.Size([10, 3, 32])\n",
      "mean pool torch.Size([10, 3, 16])\n",
      "x_a after encoding torch.Size([10, 3, 16])\n",
      "x_l after encoding torch.Size([10, 1, 16])\n",
      "x after concat torch.Size([10, 4, 16])\n",
      "after encoding torch.Size([10, 4, 16])\n",
      "Shape of output: torch.Size([10, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = MasterNet()\n",
    "acoustic_features = torch.randn(10, 40, 17) # samples x features (or channels) x N (relative duration)\n",
    "lexical_features = torch.randn(10, 1, 300)\n",
    "visual_features = None\n",
    "output = net(visual_features, acoustic_features, lexical_features)\n",
    "print(f'Shape of output: {output.shape}')\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-gardening",
   "metadata": {},
   "source": [
    "# Acoustic Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-riverside",
   "metadata": {},
   "source": [
    "Inputs for acoustic branch will be N x 40 where N [1,33]  \n",
    "Time step: (2, 10) (seconds?)  \n",
    "N: relative duration after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "complex-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticNet(nn.Module):\n",
    "    def __init__(self, num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2):\n",
    "        super(AcousticNet, self).__init__()\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\n",
    "        self.convs = [self.conv1, self.conv2, self.conv3, self.conv4]\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=conv_width,hidden_size=32,num_layers=num_gru_layers) # 19 is hardcoded\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"initial x_a shape\",x.shape)\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.relu(self.max_pool(self.convs[i](x)))\n",
    "        print('after final conv', x.shape)\n",
    "        x = torch.transpose(x, 1, 2) \n",
    "        print('after transpose', x.shape)\n",
    "        x, _ = self.gru(x)\n",
    "        print('gru', x.shape)\n",
    "        x = self.mean_pool(x)\n",
    "        print('mean pool', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "mounted-broadway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial x_a shape torch.Size([10, 40, 17])\n",
      "after final conv torch.Size([10, 128, 2])\n",
      "after transpose torch.Size([10, 2, 128])\n",
      "gru torch.Size([10, 2, 32])\n",
      "mean pool torch.Size([10, 2, 16])\n",
      "Shape of output: torch.Size([10, 2, 16])\n"
     ]
    }
   ],
   "source": [
    "# Test dummy input\n",
    "net = AcousticNet(num_conv_layers = 4, num_gru_layers = 3, kernel_size = 3, conv_width = 128)\n",
    "test_vec = torch.randn(10, 40, 17) # samples x features (or channels) x N (relative duration)\n",
    "output = net(test_vec)\n",
    "print(f'Shape of output: {output.shape}')\n",
    "# assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-inclusion",
   "metadata": {},
   "source": [
    "# Lexical Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "heated-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement GRU (or transformer)\n",
    "class LexicalNet(nn.Module):\n",
    "    def __init__(self, num_gru_layers = 2):\n",
    "        super(LexicalNet, self).__init__()\n",
    "        # implement GRU (or transformer)\n",
    "        self.gru = nn.GRU(input_size=300,hidden_size=32,num_layers=num_gru_layers)\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.mean_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "juvenile-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dummy input\n",
    "net = LexicalNet(num_gru_layers = 3)\n",
    "test_vec = torch.randn(10, 1, 300)\n",
    "output = net(test_vec)\n",
    "assert output.shape[-1] == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-anchor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
